#!/usr/bin/env python3
"""
Digest Agent Core - è®ºæ–‡æ•´ç†æ ¸å¿ƒ Agentï¼ˆä½œä¸º sub-agentï¼‰

åŠŸèƒ½ï¼š
1. æ¥æ”¶å¤šç§è¾“å…¥æºï¼ˆXHS URLã€PDF URLã€PDF æœ¬åœ°è·¯å¾„ï¼‰
2. ä¸‹è½½ PDF å¹¶è¯»å–å…¨æ–‡å†…å®¹
3. ç”Ÿæˆç»“æ„åŒ–è®ºæ–‡æ•´ç†
4. ä¿å­˜åˆ° Notion æ•°æ®åº“

ä½œä¸º sub-agent è¢«ä¸» paper_agent è°ƒç”¨
"""

import asyncio
import os
import sys
from pathlib import Path
from typing import Annotated
import json
import httpx
import fitz  # PyMuPDF
import time

from agents import Agent, function_tool, Runner
from openai import AsyncOpenAI
from ..utils.logger import get_logger

# åˆå§‹åŒ–æ—¥å¿—
logger = get_logger(__name__)

# é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent  # spec-paper-notion-agent/
DIGEST_ROOT = Path(__file__).resolve().parent  # src/services/
OUTPUT_DIR = PROJECT_ROOT / "paper_digest" / "outputs"  # ä¿æŒè¾“å‡ºåœ¨åŸä½ç½®
PDF_DIR = PROJECT_ROOT / "paper_digest" / "pdfs"  # ä¿æŒPDFåœ¨åŸä½ç½®

# ç¡®ä¿ç›®å½•å­˜åœ¨
OUTPUT_DIR.mkdir(exist_ok=True)
PDF_DIR.mkdir(exist_ok=True)

# å…¨å±€å˜é‡
_openai_client = None
_current_paper = {}


def _init_digest_globals(openai_client):
    """åˆå§‹åŒ–å…¨å±€å˜é‡"""
    global _openai_client
    _openai_client = openai_client


@function_tool
async def fetch_xiaohongshu_post(
    post_url: Annotated[str, "å°çº¢ä¹¦å¸–å­çš„å®Œæ•´URL"]
) -> str:
    """
    è·å–å°çº¢ä¹¦å¸–å­å†…å®¹

    å‚æ•°:
        post_url: å°çº¢ä¹¦å¸–å­URL

    è¿”å›:
        JSONæ ¼å¼çš„å¸–å­ä¿¡æ¯ï¼ˆåŒ…å« raw_contentï¼‰
    """
    global _current_paper
    start_time = time.time()

    # å¯¼å…¥ xiaohongshu æœåŠ¡
    from .xiaohongshu import XiaohongshuClient

    try:
        logger.info("ğŸ” å¼€å§‹è·å–å°çº¢ä¹¦å¸–å­")
        client = XiaohongshuClient(
            cookies=os.getenv("XHS_COOKIES"),
            openai_client=_openai_client  # âœ¨ ä¼ é€’ OpenAI client ç”¨äº LLM è§£æ
        )
        post = await client.fetch_post(post_url)

        _current_paper = {
            "post_id": post.post_id,
            "post_url": str(post.post_url),
            "blogger_name": post.blogger_name,
            "raw_content": post.raw_content,
        }

        elapsed = time.time() - start_time
        logger.info(
            "âœ… å°çº¢ä¹¦å¸–å­è·å–æˆåŠŸ",
            post_id=post.post_id,
            content_length=len(post.raw_content),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "post_id": post.post_id,
            "content": post.raw_content,
            "message": f"âœ… å¸–å­å†…å®¹è·å–æˆåŠŸï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ å°çº¢ä¹¦å¸–å­è·å–å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"è·å–å¸–å­å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def extract_paper_metadata(
    xiaohongshu_content: Annotated[str, "å°çº¢ä¹¦å¸–å­å†…å®¹ï¼ˆå¯é€‰ï¼‰"] = "",
    pdf_content: Annotated[str, "PDFçš„æ–‡æœ¬å†…å®¹"] = "",
    pdf_metadata: Annotated[str, "PDFçš„å…ƒæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"] = ""
) -> str:
    """
    ä» PDF å’Œå°çº¢ä¹¦å†…å®¹ä¸­æå–æ‰€æœ‰è®ºæ–‡ä¿¡æ¯ï¼ˆä¸€æ¬¡ LLM è°ƒç”¨ï¼‰

    å‚æ•°:
        xiaohongshu_content: å°çº¢ä¹¦å¸–å­å†…å®¹ï¼ˆå¯é€‰ï¼‰
        pdf_content: PDF æ–‡æœ¬å†…å®¹
        pdf_metadata: PDF å…ƒæ•°æ® (JSONæ ¼å¼)

    è¿”å›:
        JSONæ ¼å¼çš„å®Œæ•´è®ºæ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - title: è®ºæ–‡è‹±æ–‡æ ‡é¢˜
        - authors: ä½œè€…åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
        - publication_date: å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        - venue: æœŸåˆŠ/ä¼šè®®åç§°
        - abstract: æ‘˜è¦
        - affiliations: æœºæ„
        - keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
        - doi: DOI
        - arxiv_id: ArXiv ID
        - project_page: é¡¹ç›®ä¸»é¡µ
        - other_resources: å…¶ä»–èµ„æº
    """
    global _openai_client, _current_paper
    start_time = time.time()

    try:
        logger.info("ğŸ“š å¼€å§‹æå–è®ºæ–‡å…ƒæ•°æ®ï¼ˆLLM è°ƒç”¨ 1/2ï¼‰")
        prompt = f"""ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å®Œæ•´çš„è®ºæ–‡ä¿¡æ¯ã€‚

# PDF å…ƒæ•°æ®
{pdf_metadata}

# PDF å†…å®¹ï¼ˆå‰5000å­—ï¼‰
{pdf_content[:5000] if pdf_content else "[æœªæä¾›]"}

# å°çº¢ä¹¦å†…å®¹ï¼ˆå‚è€ƒï¼‰
{xiaohongshu_content[:2000] if xiaohongshu_content else "[æœªæä¾›]"}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¿…é¡»è¿”å› JSON æ ¼å¼ï¼š

{{
    "title": "è®ºæ–‡è‹±æ–‡æ ‡é¢˜ï¼ˆå¿…å¡«ï¼‰",
    "authors": ["ä½œè€…1", "ä½œè€…2"],
    "publication_date": "YYYY-MM-DDï¼ˆå¦‚æœåªæœ‰å¹´ä»½ï¼Œä½¿ç”¨ YYYY-01-01ï¼‰",
    "venue": "æœŸåˆŠ/ä¼šè®®åç§°",
    "abstract": "è‹±æ–‡æ‘˜è¦",
    "affiliations": "Stanford University; MIT",
    "keywords": ["keyword1", "keyword2", "tag1"],
    "doi": "10.1234/exampleï¼ˆå¦‚æœæœ‰ï¼‰",
    "arxiv_id": "2410.xxxxxï¼ˆå¦‚æœæ˜¯ arXiv è®ºæ–‡ï¼‰",
    "project_page": "é¡¹ç›®ä¸»é¡µé“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰",
    "other_resources": "ä»£ç ä»“åº“ã€æ•°æ®é›†ç­‰ï¼ˆå¯ç”¨åˆ†å·åˆ†éš”ï¼‰"
}}

âš ï¸ è¦æ±‚ï¼š
1. title å¿…é¡»æå–ï¼Œå…¶ä»–å­—æ®µæ²¡æœ‰ä¿¡æ¯å¯ä»¥è®¾ä¸º null
2. publication_date å¿…é¡»æ˜¯å®Œæ•´çš„ YYYY-MM-DD æ ¼å¼
3. authors å’Œ keywords å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼
4. å…¶ä»–å­—æ®µå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„ï¼Œè®¾ç½®ä¸ºå¯ç”¨çš„æ ¼å¼
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œä½¿ç”¨ null å€¼
"""

        # ä½¿ç”¨ Agent æ›¿ä»£ç›´æ¥çš„ LLM è°ƒç”¨
        metadata_extraction_agent = Agent(
            name="metadata_extraction_agent",
            instructions="ä½ æ˜¯ä¸“ä¸šçš„è®ºæ–‡ä¿¡æ¯æå–ä¸“å®¶ã€‚ä½ å¿…é¡»å‡†ç¡®å®Œæ•´åœ°æå–è®ºæ–‡çš„æ‰€æœ‰å…ƒæ•°æ®ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·çš„è¦æ±‚ï¼Œä»¥ JSON æ ¼å¼è¿”å›æå–çš„ä¿¡æ¯ã€‚",
            model="gpt-5-mini",
        )

        result = await Runner.run(
            starting_agent=metadata_extraction_agent,
            input=prompt,
            max_turns=1
        )

        # æå–Agentè¿”å›çš„æ–‡æœ¬å†…å®¹
        response_text = result.final_output if hasattr(result, 'final_output') else str(result)

        # å°è¯•è§£æ JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()

        extracted_info = json.loads(response_text)

        # éªŒè¯å¿…å¡«å­—æ®µ
        if not extracted_info.get("title"):
            extracted_info["title"] = "Unknown Paper"

        _current_paper.update(extracted_info)

        # å¦‚æœ PDF å·²ä¸‹è½½ä½†æ ‡é¢˜ä¸ä¸€è‡´ï¼Œé‡æ–°æ•´ç†æ–‡ä»¶
        old_pdf_path = _current_paper.get("pdf_path")
        correct_title = extracted_info.get("title")

        if old_pdf_path and correct_title and Path(old_pdf_path).exists():
            old_path = Path(old_pdf_path)
            expected_path = _get_paper_pdf_path(correct_title)

            # å¦‚æœè·¯å¾„ä¸åŒï¼Œè¯´æ˜ä¸‹è½½æ—¶ä½¿ç”¨çš„æ˜¯é”™è¯¯çš„æ ‡é¢˜
            if old_path != expected_path:
                try:
                    logger.info(
                        "ğŸ“ æ£€æµ‹åˆ°æ ‡é¢˜ä¸ä¸€è‡´ï¼Œé‡æ–°æ•´ç† PDF æ–‡ä»¶",
                        old_path=str(old_path),
                        new_path=str(expected_path)
                    )

                    # åˆ›å»ºæ–°ç›®å½•
                    expected_path.parent.mkdir(parents=True, exist_ok=True)

                    # ç§»åŠ¨ PDF æ–‡ä»¶
                    import shutil
                    shutil.move(str(old_path), str(expected_path))

                    # åˆ é™¤æ—§ç›®å½•ï¼ˆå¦‚æœä¸ºç©ºï¼‰
                    try:
                        if old_path.parent != expected_path.parent:
                            old_path.parent.rmdir()
                    except:
                        pass  # ç›®å½•ä¸ä¸ºç©ºï¼Œå¿½ç•¥

                    # æ›´æ–°å…¨å±€å˜é‡ä¸­çš„è·¯å¾„
                    _current_paper["pdf_path"] = str(expected_path)

                    logger.info("âœ… PDF æ–‡ä»¶å·²é‡æ–°æ•´ç†åˆ°æ­£ç¡®è·¯å¾„")

                except Exception as e:
                    logger.warning(f"é‡æ–°æ•´ç† PDF æ–‡ä»¶å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨åŸè·¯å¾„")

        elapsed = time.time() - start_time
        logger.info(
            "âœ… è®ºæ–‡å…ƒæ•°æ®æå–æˆåŠŸ",
            title=extracted_info.get("title", "Unknown"),
            authors_count=len(extracted_info.get("authors", [])),
            keywords_count=len(extracted_info.get("keywords", [])),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            **extracted_info,
            "message": f"âœ… è®ºæ–‡ä¿¡æ¯æå–æˆåŠŸï¼ˆæ ‡é¢˜ + å…ƒæ•°æ®ï¼‰ï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ è®ºæ–‡å…ƒæ•°æ®æå–å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"æå–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def search_arxiv_pdf(
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜"]
) -> str:
    """
    åœ¨ arXiv æœç´¢è®ºæ–‡ PDF é“¾æ¥

    å‚æ•°:
        paper_title: è®ºæ–‡æ ‡é¢˜

    è¿”å›:
        JSONæ ¼å¼çš„PDFé“¾æ¥ä¿¡æ¯
    """
    start_time = time.time()
    try:
        import urllib.parse

        logger.info("ğŸ” å¼€å§‹åœ¨ arXiv æœç´¢è®ºæ–‡", paper_title=paper_title[:100])

        # arXiv API æœç´¢
        query = urllib.parse.quote(paper_title)
        api_url = f"http://export.arxiv.org/api/query?search_query=ti:{query}&max_results=3"

        proxy = os.getenv('http_proxy')
        mounts = None
        if proxy:
            mounts = {
                "http://": httpx.AsyncHTTPTransport(proxy=proxy),
                "https://": httpx.AsyncHTTPTransport(proxy=proxy),
            }

        async with httpx.AsyncClient(timeout=30.0, mounts=mounts) as client:
            response = await client.get(api_url)
            response.raise_for_status()

            # è§£æ XML å“åº”
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)

            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„è®ºæ–‡
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entries = root.findall('atom:entry', ns)

            if entries:
                entry = entries[0]
                # è·å– arXiv ID
                arxiv_id_elem = entry.find('atom:id', ns)
                if arxiv_id_elem is not None:
                    arxiv_id_full = arxiv_id_elem.text
                    # æå– ID éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼šhttp://arxiv.org/abs/2410.04618 -> 2410.04618ï¼‰
                    arxiv_id = arxiv_id_full.split('/abs/')[-1]
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"

                    # è·å–æ ‡é¢˜ç¡®è®¤
                    title_elem = entry.find('atom:title', ns)
                    found_title = title_elem.text.strip() if title_elem is not None else "Unknown"

                    elapsed = time.time() - start_time
                    logger.info(
                        "âœ… arXiv æœç´¢æˆåŠŸ",
                        arxiv_id=arxiv_id,
                        found_title=found_title[:100],
                        elapsed_time=f"{elapsed:.2f}s"
                    )

                    return json.dumps({
                        "success": True,
                        "pdf_url": pdf_url,
                        "arxiv_id": arxiv_id,
                        "arxiv_abs_url": f"https://arxiv.org/abs/{arxiv_id}",
                        "found_title": found_title,
                        "message": f"âœ… åœ¨ arXiv æ‰¾åˆ°è®ºæ–‡ï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰\nPDF: {pdf_url}\narXiv ID: {arxiv_id}"
                    }, ensure_ascii=False, indent=2)

        # æœªæ‰¾åˆ°
        elapsed = time.time() - start_time
        logger.warning(
            "âš ï¸ arXiv æœªæ‰¾åˆ°è®ºæ–‡",
            paper_title=paper_title[:100],
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"åœ¨ arXiv æœªæ‰¾åˆ°è®ºæ–‡ã€Š{paper_title}ã€‹ã€‚å¯èƒ½ä¸æ˜¯ arXiv è®ºæ–‡ï¼Œæˆ–æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…ã€‚"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ arXiv æœç´¢å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"arXiv æœç´¢å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def download_pdf_from_url(
    pdf_url: Annotated[str, "PDFæ–‡ä»¶çš„URL"],
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºå‘½åæ–‡ä»¶ï¼‰"] = "paper"
) -> str:
    """
    ä¸‹è½½ PDF å¹¶è¯»å–å…¨éƒ¨å†…å®¹

    å‚æ•°:
        pdf_url: PDF æ–‡ä»¶çš„ URL
        paper_title: è®ºæ–‡æ ‡é¢˜

    è¿”å›:
        åŒ…å« PDF å†…å®¹å’Œå…ƒæ•°æ®çš„ JSON
    """
    global _current_paper
    start_time = time.time()

    try:
        logger.info("ğŸ“¥ å¼€å§‹ä¸‹è½½ PDF", pdf_url=pdf_url[:100], paper_title=paper_title[:50])

        # ä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼špaper_digest/pdfs/{Paper_Title}/paper.pdf
        local_path = _get_paper_pdf_path(paper_title)

        # ä¸‹è½½ PDF
        proxy = os.getenv('http_proxy')
        mounts = None
        if proxy:
            mounts = {
                "http://": httpx.AsyncHTTPTransport(proxy=proxy),
                "https://": httpx.AsyncHTTPTransport(proxy=proxy),
            }

        async with httpx.AsyncClient(
            timeout=60.0,
            follow_redirects=True,
            mounts=mounts
        ) as client:
            response = await client.get(pdf_url)
            response.raise_for_status()

            with open(local_path, 'wb') as f:
                f.write(response.content)

        # è¯»å– PDF å†…å®¹
        logger.info("ğŸ“– å¼€å§‹è¯»å– PDF å†…å®¹")
        pdf_content, pdf_metadata = _read_pdf_file(str(local_path))

        _current_paper["pdf_path"] = str(local_path)
        _current_paper["pdf_url"] = pdf_url
        _current_paper["pdf_content"] = pdf_content
        _current_paper["pdf_metadata"] = pdf_metadata

        elapsed = time.time() - start_time
        logger.info(
            "âœ… PDF ä¸‹è½½å¹¶è¯»å–æˆåŠŸ",
            file_size=f"{len(response.content) / 1024 / 1024:.2f}MB",
            pages=pdf_metadata.get("pages", 0),
            content_length=len(pdf_content),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "local_path": str(local_path),
            "pdf_url": pdf_url,
            "pdf_content": pdf_content[:5000],  # è¿”å›å‰5000å­—ç¬¦é¢„è§ˆ
            "pdf_metadata": json.dumps(pdf_metadata, ensure_ascii=False),
            "message": f"âœ… PDF ä¸‹è½½å¹¶è¯»å–æˆåŠŸï¼æ–‡ä»¶: {local_path}ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ PDF ä¸‹è½½å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"å¤„ç†å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def read_local_pdf(
    pdf_path: Annotated[str, "PDFæ–‡ä»¶çš„æœ¬åœ°è·¯å¾„"]
) -> str:
    """
    è¯»å–æœ¬åœ° PDF æ–‡ä»¶å†…å®¹

    å‚æ•°:
        pdf_path: PDF æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„

    è¿”å›:
        åŒ…å« PDF å†…å®¹å’Œå…ƒæ•°æ®çš„ JSON
    """
    global _current_paper
    start_time = time.time()

    try:
        logger.info("ğŸ“– å¼€å§‹è¯»å–æœ¬åœ° PDF", pdf_path=pdf_path)

        pdf_content, pdf_metadata = _read_pdf_file(pdf_path)

        _current_paper["pdf_path"] = pdf_path
        _current_paper["pdf_content"] = pdf_content
        _current_paper["pdf_metadata"] = pdf_metadata

        elapsed = time.time() - start_time
        file_size = os.path.getsize(pdf_path) / 1024 / 1024 if os.path.exists(pdf_path) else 0
        logger.info(
            "âœ… æœ¬åœ° PDF è¯»å–æˆåŠŸ",
            pdf_path=pdf_path,
            file_size=f"{file_size:.2f}MB",
            pages=pdf_metadata.get("pages", 0),
            content_length=len(pdf_content),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "pdf_path": pdf_path,
            "pdf_content": pdf_content[:5000],
            "pdf_metadata": json.dumps(pdf_metadata, ensure_ascii=False),
            "message": f"âœ… PDF è¯»å–æˆåŠŸï¼æ–‡ä»¶: {pdf_path}ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ æœ¬åœ° PDF è¯»å–å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"è¯»å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


def _get_paper_directory(paper_title: str) -> Path:
    """
    ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºç‹¬ç«‹çš„ç›®å½•

    ç›®å½•ç»“æ„ï¼š
    paper_digest/pdfs/
    â”œâ”€â”€ Paper_Title_1/
    â”‚   â”œâ”€â”€ Paper_Title_1.pdf
    â”‚   â””â”€â”€ extracted_images/
    â”œâ”€â”€ Paper_Title_2/
    â”‚   â”œâ”€â”€ Paper_Title_2.pdf
    â”‚   â””â”€â”€ extracted_images/

    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜ï¼ˆå®Œæ•´æ ‡é¢˜ï¼Œä¸æˆªæ–­ï¼‰

    Returns:
        è®ºæ–‡ç›®å½•çš„ Path å¯¹è±¡
    """
    # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™å®Œæ•´é•¿åº¦
    safe_title = paper_title.replace('/', '_').replace(':', '_').replace('?', '_').replace('\\', '_').strip()
    # é™åˆ¶æœ€å¤§é•¿åº¦ä¸º 150 å­—ç¬¦ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿé™åˆ¶ï¼ˆé€šå¸¸ 255ï¼‰
    safe_title = safe_title[:150]
    paper_dir = PDF_DIR / safe_title
    paper_dir.mkdir(parents=True, exist_ok=True)

    return paper_dir


def _get_paper_pdf_path(paper_title: str) -> Path:
    """è·å–è®ºæ–‡ PDF çš„ä¿å­˜è·¯å¾„

    è¿”å›æ ¼å¼: paper_digest/pdfs/{Paper_Title}/{Paper_Title}.pdf

    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜ï¼ˆå®Œæ•´æ ‡é¢˜ï¼‰
    """
    paper_dir = _get_paper_directory(paper_title)
    # æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰ï¼Œä½¿ç”¨ä¸ç›®å½•ç›¸åŒçš„åç§°
    safe_filename = paper_title.replace('/', '_').replace(':', '_').replace('?', '_').replace('\\', '_').strip()
    safe_filename = safe_filename[:150]  # ä¸ç›®å½•åä¿æŒä¸€è‡´
    return paper_dir / f"{safe_filename}.pdf"


def _get_paper_images_dir(paper_title: str) -> Path:
    """è·å–è®ºæ–‡å›¾ç‰‡æå–ç›®å½•çš„è·¯å¾„"""
    paper_dir = _get_paper_directory(paper_title)
    images_dir = paper_dir / "extracted_images"
    images_dir.mkdir(parents=True, exist_ok=True)
    return images_dir


def _read_pdf_file(pdf_path: str):
    """è¯»å– PDF æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    doc = fitz.open(pdf_path)
    total_pages = doc.page_count

    # æå–å…ƒæ•°æ®
    metadata = doc.metadata
    metadata_dict = {
        "title": metadata.get("title", ""),
        "author": metadata.get("author", ""),
        "subject": metadata.get("subject", ""),
        "keywords": metadata.get("keywords", ""),
        "creator": metadata.get("creator", ""),
        "producer": metadata.get("producer", ""),
        "creationDate": metadata.get("creationDate", ""),
        "modDate": metadata.get("modDate", ""),
        "pages": total_pages,
    }

    # åˆ†æ‰¹è¯»å–å…¨éƒ¨å†…å®¹ï¼ˆæ¯æ¬¡10é¡µï¼‰
    PAGES_PER_BATCH = 10
    all_pdf_text = []
    current_page = 0

    while current_page < total_pages:
        end_page = min(current_page + PAGES_PER_BATCH, total_pages)
        batch_text = ""

        for page_num in range(current_page, end_page):
            page = doc[page_num]
            batch_text += f"\n\n--- Page {page_num + 1} ---\n\n"
            batch_text += page.get_text()

        all_pdf_text.append(batch_text)
        current_page = end_page

    doc.close()

    # åˆå¹¶æ‰€æœ‰å†…å®¹
    full_pdf_content = "".join(all_pdf_text)

    # æˆªæ–­åˆ°åˆç†é•¿åº¦ï¼ˆ~50000å­—ç¬¦ï¼‰
    MAX_CHARS = 50000
    if len(full_pdf_content) > MAX_CHARS:
        pdf_content = full_pdf_content[:MAX_CHARS] + "\n\n[å†…å®¹å·²æˆªæ–­ï¼Œåç»­å†…å®¹ç•¥]"
    else:
        pdf_content = full_pdf_content

    return pdf_content, metadata_dict


def _auto_insert_images(
    markdown_content: str,
    extracted_images: list,
    relative_image_path: str
) -> str:
    """
    è‡ªåŠ¨å°†å›¾ç‰‡æ’å…¥åˆ° Markdown çš„é€‚å½“ä½ç½®

    æ”¹è¿›ç­–ç•¥ï¼š
    1. æ£€æŸ¥ Markdown æ˜¯å¦å·²ç»åŒ…å«å›¾ç‰‡å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼Œåªæ’å…¥ç¼ºå¤±çš„ï¼‰
    2. æ›´æ™ºèƒ½çš„ç« èŠ‚åŒ¹é…ï¼š
       - åˆ†æ caption å’Œå›¾ç‰‡ç¼–å·
       - Figure 1-2 é€šå¸¸æ˜¯æ–¹æ³•/æ¶æ„å›¾ â†’ æ–¹æ³•ç« èŠ‚
       - Table å’Œåç»­ Figures é€šå¸¸æ˜¯å®éªŒç»“æœ â†’ å®éªŒç« èŠ‚
       - ç‰¹æ®Š Figuresï¼ˆtraining, ablationï¼‰â†’ ç›¸åº”ç« èŠ‚
    3. æŒ‰ç¼–å·é¡ºåºæ’å…¥ï¼Œç¡®ä¿ä¸é—æ¼

    Args:
        markdown_content: åŸå§‹ Markdown å†…å®¹
        extracted_images: æå–çš„å›¾ç‰‡åˆ—è¡¨
        relative_image_path: å›¾ç‰‡ç›¸å¯¹è·¯å¾„

    Returns:
        æ’å…¥å›¾ç‰‡åçš„ Markdown å†…å®¹
    """
    import re

    # æ£€æŸ¥å·²æ’å…¥çš„å›¾ç‰‡
    existing_images = set()
    for match in re.finditer(r'<img src="[^"]*?/([^/"]+)"', markdown_content):
        existing_images.add(match.group(1))

    # è¿‡æ»¤å‡ºéœ€è¦æ’å…¥çš„å›¾ç‰‡
    images_to_insert = [img for img in extracted_images if img['filename'] not in existing_images]

    if not images_to_insert:
        logger.info("æ‰€æœ‰å›¾ç‰‡å·²æ’å…¥ï¼Œæ— éœ€å¤„ç†")
        return markdown_content

    logger.info(f"éœ€è¦æ’å…¥ {len(images_to_insert)} å¼ å›¾ç‰‡")

    # æŒ‰ Figure/Table ç¼–å·æ’åº
    def sort_key(img):
        fig_type = img.get('fig_type', 'Figure')
        fig_name = img.get('fig_name', '0')
        try:
            num = int(fig_name)
        except:
            num = 999
        # Table ä¼˜å…ˆçº§ä½äº Figure
        return (0 if fig_type == 'Figure' else 1, num)

    images_to_insert.sort(key=sort_key)

    # æ™ºèƒ½åˆ†ç»„ï¼šæ ¹æ®å†…å®¹å’Œç¼–å·
    method_images = []
    experiment_images = []
    other_images = []

    for img in images_to_insert:
        caption = img.get('caption', '').lower()
        fig_type = img.get('fig_type', 'Figure')
        fig_name = img.get('fig_name', '0')

        try:
            fig_num = int(fig_name)
        except:
            fig_num = 999

        # åˆ†ç±»è§„åˆ™
        if fig_type == 'Figure' and fig_num <= 2:
            # Figure 1-2 é€šå¸¸æ˜¯æ–¹æ³•/æ¶æ„å›¾
            method_images.append(img)
        elif 'method' in caption or 'architecture' in caption or 'framework' in caption or 'mechanism' in caption or 'optimization' in caption:
            method_images.append(img)
        elif 'performance' in caption or 'result' in caption or 'comparison' in caption or 'experiment' in caption or 'training' in caption or fig_type == 'Table':
            experiment_images.append(img)
        else:
            other_images.append(img)

    # ç”Ÿæˆå›¾ç‰‡ HTML
    def create_image_html(images):
        html = "\n\n"
        for img in images:
            fig_type = img.get('fig_type', 'Figure')
            fig_name = img.get('fig_name', '')
            filename = img['filename']
            caption = img.get('caption', '')
            html += f'''<figure>
  <img src="{relative_image_path}/{filename}" alt="{fig_type} {fig_name}">
  <figcaption>{caption}</figcaption>
</figure>

'''
        return html

    modified_content = markdown_content

    # æ’å…¥æ–¹æ³•ç›¸å…³å›¾ç‰‡åˆ° "æ–¹æ³•å®ç°ç»†èŠ‚" ç« èŠ‚
    if method_images:
        method_pattern = r'(##\s*âš™ï¸\s*æ–¹æ³•å®ç°ç»†èŠ‚.*?)((?=\n##)|$)'
        if re.search(method_pattern, modified_content, re.DOTALL):
            images_html = create_image_html(method_images)
            def replacer(match):
                section_content = match.group(1)
                next_section = match.group(2)
                return section_content + images_html + next_section
            modified_content = re.sub(method_pattern, replacer, modified_content, flags=re.DOTALL, count=1)
            logger.info(f"æ’å…¥ {len(method_images)} å¼ å›¾ç‰‡åˆ°æ–¹æ³•ç« èŠ‚")
        else:
            # å¦‚æœæ²¡æœ‰"æ–¹æ³•å®ç°ç»†èŠ‚"ï¼Œå°è¯•"æœ¬æ–‡æ–¹æ³•"
            method_pattern2 = r'(##\s*ğŸ’¡\s*æœ¬æ–‡æ–¹æ³•.*?)((?=\n##)|$)'
            if re.search(method_pattern2, modified_content, re.DOTALL):
                images_html = create_image_html(method_images)
                def replacer(match):
                    section_content = match.group(1)
                    next_section = match.group(2)
                    return section_content + images_html + next_section
                modified_content = re.sub(method_pattern2, replacer, modified_content, flags=re.DOTALL, count=1)
                logger.info(f"æ’å…¥ {len(method_images)} å¼ å›¾ç‰‡åˆ°æœ¬æ–‡æ–¹æ³•ç« èŠ‚")
            else:
                experiment_images.extend(method_images)  # æ”¾åˆ°å®éªŒç« èŠ‚

    # æ’å…¥å®éªŒç›¸å…³å›¾ç‰‡åˆ° "å®éªŒä¸ç»“æœ" ç« èŠ‚
    if experiment_images:
        result_pattern = r'(##\s*ğŸ“Š\s*å®éªŒä¸ç»“æœ.*?)((?=\n##)|$)'
        if re.search(result_pattern, modified_content, re.DOTALL):
            images_html = create_image_html(experiment_images)
            def replacer(match):
                section_content = match.group(1)
                next_section = match.group(2)
                return section_content + images_html + next_section
            modified_content = re.sub(result_pattern, replacer, modified_content, flags=re.DOTALL, count=1)
            logger.info(f"æ’å…¥ {len(experiment_images)} å¼ å›¾ç‰‡åˆ°å®éªŒç« èŠ‚")
        else:
            # å¦‚æœæ²¡æœ‰å®éªŒç« èŠ‚ï¼Œæ·»åŠ åˆ°æ–‡æ¡£æœ«å°¾
            images_html = create_image_html(experiment_images)
            modified_content += "\n\n---\n\n## ğŸ“Š Figures & Tables\n\n" + images_html
            logger.info(f"æ’å…¥ {len(experiment_images)} å¼ å›¾ç‰‡åˆ°æ–‡æ¡£æœ«å°¾")

    # å…¶ä»–å›¾ç‰‡æ’å…¥åˆ°æ–‡æ¡£æœ«å°¾
    if other_images:
        images_html = create_image_html(other_images)
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ "Figures & Tables" ç« èŠ‚
        if "## ğŸ“Š Figures & Tables" in modified_content:
            # è¿½åŠ åˆ°è¯¥ç« èŠ‚
            modified_content = modified_content.replace(
                "## ğŸ“Š Figures & Tables\n\n",
                f"## ğŸ“Š Figures & Tables\n\n{images_html}"
            )
        else:
            modified_content += "\n\n---\n\n## ğŸ“Š Other Figures\n\n" + images_html
        logger.info(f"æ’å…¥ {len(other_images)} å¼ å…¶ä»–å›¾ç‰‡")

    return modified_content






@function_tool
async def generate_paper_digest(
    xiaohongshu_content: Annotated[str, "å°çº¢ä¹¦å¸–å­å†…å®¹"] = "",
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜"] = "",
    pdf_content: Annotated[str, "PDFå…¨æ–‡å†…å®¹"] = "",
    authors: Annotated[str, "ä½œè€…åˆ—è¡¨ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²ï¼‰"] = "[]",
    publication_date: Annotated[str, "å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰"] = "",
    venue: Annotated[str, "æœŸåˆŠ/ä¼šè®®åç§°"] = "",
    abstract: Annotated[str, "æ‘˜è¦"] = "",
    affiliations: Annotated[str, "æœºæ„"] = "",
    keywords: Annotated[str, "å…³é”®è¯åˆ—è¡¨ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²ï¼‰"] = "[]",
    project_page: Annotated[str, "é¡¹ç›®ä¸»é¡µ"] = "",
    other_resources: Annotated[str, "å…¶ä»–èµ„æº"] = "",
    pdf_path: Annotated[str, "PDFæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæå–å›¾ç‰‡ï¼Œå¯é€‰ï¼‰"] = "",
) -> str:
    """
    ç”Ÿæˆç»“æ„åŒ–è®ºæ–‡æ•´ç†

    å‚æ•°:
        xiaohongshu_content: å°çº¢ä¹¦å¸–å­å†…å®¹
        paper_title: è®ºæ–‡æ ‡é¢˜
        pdf_content: PDF å…¨æ–‡å†…å®¹
        authors: ä½œè€…åˆ—è¡¨
        publication_date: å‘è¡¨æ—¥æœŸ
        venue: æœŸåˆŠ/ä¼šè®®
        abstract: æ‘˜è¦
        affiliations: æœºæ„
        keywords: å…³é”®è¯
        project_page: é¡¹ç›®ä¸»é¡µ
        other_resources: å…¶ä»–èµ„æº

    è¿”å›:
        Markdownæ ¼å¼çš„è®ºæ–‡æ•´ç†
    """
    global _openai_client, _current_paper
    start_time = time.time()

    # è¯»å–æ¨¡æ¿
    template_path = Path(__file__).parent / "digest_template.md"
    with open(template_path, 'r', encoding='utf-8') as f:
        template_content = f.read()

    # æå– PDF ä¸­çš„å›¾ç‰‡ï¼ˆå¦‚æœæä¾›äº† PDF è·¯å¾„ï¼‰
    images_info = ""

    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ pdf_pathï¼Œå¦‚æœä¸ºç©ºåˆ™ä»å…¨å±€å˜é‡è·å–
    effective_pdf_path = pdf_path
    if not effective_pdf_path or not Path(effective_pdf_path).exists():
        effective_pdf_path = _current_paper.get("pdf_path", "")
        if effective_pdf_path:
            logger.info("ğŸ“„ ä½¿ç”¨å…¨å±€å˜é‡ä¸­çš„ PDF è·¯å¾„", pdf_path=effective_pdf_path[:100])

    if effective_pdf_path and Path(effective_pdf_path).exists():
        try:
            logger.info("ğŸ–¼ï¸  å¼€å§‹æå– PDF ä¸­çš„ Figures/Tables", pdf_path=effective_pdf_path[:100])
            from .pdf_figure_extractor_v2 import PDFFigureExtractorV2

            # å°†å›¾ç‰‡ä¿å­˜åˆ°è®ºæ–‡ç‰¹å®šç›®å½•ï¼špaper_digest/pdfs/{Paper_Title}/extracted_images/
            images_dir = _get_paper_images_dir(paper_title)

            extractor = PDFFigureExtractorV2(str(images_dir))
            images, blocks = extractor.extract(effective_pdf_path)

            if images:
                # V2 æå–å™¨å·²ç»æä¾›äº†å®Œæ•´çš„ Figures/Tablesï¼Œä¸éœ€è¦å†é€‰æ‹©
                # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æå–çš„å›¾ç‰‡ï¼ˆå·²æŒ‰é‡è¦æ€§æ’åºï¼‰

                # æ ¼å¼åŒ–å›¾ç‰‡ä¿¡æ¯ä¾› LLM ä½¿ç”¨ï¼ˆè¯¦ç»†ç‰ˆï¼ŒåŒ…å«å®Œæ•´ captionï¼‰
                images_list = "\n".join([
                    f"ã€{img['fig_type']} {img['fig_name']}ã€‘\n" +
                    f"  æ–‡ä»¶å: {img['filename']}\n" +
                    f"  Caption: {img.get('caption', '(æ— caption)') or '(æ— caption)'}\n" +
                    f"  é¡µç : ç¬¬ {img['page']} é¡µ"
                    for img in images
                ])

                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼ˆä» outputs/ åˆ° pdfs/{Paper_Title}/extracted_images/ï¼‰
                safe_title = paper_title.replace('/', '_').replace(':', '_').replace('?', '_').replace('\\', '_').strip()[:150]
                relative_image_path = f"../pdfs/{safe_title}/extracted_images"

                # ç»Ÿè®¡æå–æ¥æº
                pdffigures2_count = sum(1 for img in images if img.get('source') == 'pdffigures2')
                python_count = sum(1 for img in images if img.get('source') == 'python_fallback')

                images_info = f"""
# è®ºæ–‡ Figures/Tablesï¼ˆå…± {len(images)} ä¸ªï¼Œå·²æå–ï¼‰

æå–æ¥æºï¼š
- PDFFigures2: {pdffigures2_count} ä¸ª ğŸ“Š
- Python Fallback: {python_count} ä¸ª ğŸ

## å›¾ç‰‡åˆ—è¡¨ï¼ˆåŒ…å«å®Œæ•´ Captionï¼‰

{images_list}

---

## ğŸ¯ å›¾ç‰‡æ’å…¥è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼ï¼‰

ä½ **å¿…é¡»**æ ¹æ®æ¯å¼ å›¾ç‰‡çš„ **Caption å†…å®¹**æ™ºèƒ½å†³å®šæ’å…¥ä½ç½®ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å…¨éƒ¨å †åˆ°ä¸€ä¸ªç« èŠ‚ã€‚

### æ’å…¥æ ¼å¼ï¼ˆâš ï¸ å¿…é¡»ç»Ÿä¸€ä½¿ç”¨ HTML <figure> æ ‡ç­¾ï¼‰
```html
<figure>
  <img src="{relative_image_path}/{{filename}}" alt="{{fig_type}} {{fig_name}}">
  <figcaption>{{å®Œæ•´captionåŸæ–‡}}</figcaption>
</figure>
```

**é‡è¦æç¤ºï¼š**
- âŒ ä¸è¦ä½¿ç”¨ Markdown å›¾ç‰‡è¯­æ³• `![alt](path)`
- âœ… å¿…é¡»ä½¿ç”¨ä¸Šé¢çš„ HTML `<figure>` æ ‡ç­¾æ ¼å¼
- âœ… ç¡®ä¿è·¯å¾„å®Œæ•´ï¼š`{relative_image_path}/{{filename}}`

### æ™ºèƒ½é€‰æ‹©ä¸æ’å…¥ç­–ç•¥ï¼ˆåŸºäºé‡è¦æ€§è¯„åˆ†ï¼‰

âš ï¸ **é‡è¦ï¼šä¸è¦æ’å…¥æ‰€æœ‰å›¾ç‰‡ï¼æ ¹æ®å›¾ç‰‡çš„é‡è¦æ€§è¯„åˆ†ï¼Œåªæ’å…¥é«˜ä»·å€¼çš„å›¾ç‰‡ã€‚**

#### è¯„åˆ†æ ‡å‡†ï¼ˆæ»¡åˆ†10åˆ†ï¼Œâ‰¥7åˆ†çš„å›¾ç‰‡æ‰æ’å…¥ï¼‰ï¼š

**è¯„åˆ†ç»´åº¦**ï¼š
1. **æ ¸å¿ƒæ–¹æ³•ç†è§£** (0-4åˆ†)
   - 4åˆ†ï¼šæ ¸å¿ƒæ¶æ„å›¾ã€ç®—æ³•æµç¨‹å›¾ã€æ–¹æ³•ç¤ºæ„å›¾
   - 3åˆ†ï¼šæ–¹æ³•åº”ç”¨ç¤ºä¾‹ã€å…³é”®ç»„ä»¶å›¾ã€**èƒŒæ™¯/åŠ¨æœºç¤ºä¾‹å›¾**
   - 2åˆ†ï¼šè¾…åŠ©ç¤ºæ„å›¾
   - 0-1åˆ†ï¼šè¾¹ç¼˜æ€§ç¤ºä¾‹

2. **å®éªŒä»·å€¼** (0-4åˆ†)
   - 4åˆ†ï¼šä¸»è¦æ€§èƒ½å¯¹æ¯”è¡¨ã€æ ¸å¿ƒå®éªŒç»“æœ
   - 3åˆ†ï¼šå…³é”®æ¶ˆèå®éªŒã€é‡è¦å¯¹æ¯”å›¾
   - 2åˆ†ï¼šæ¬¡è¦å®éªŒç»“æœã€**èƒŒæ™¯ç¤ºä¾‹çš„è¯´æ˜ä»·å€¼**
   - 0-1åˆ†ï¼šè®­ç»ƒæ›²çº¿ã€è¡Œä¸ºç»Ÿè®¡ã€ç»†èŠ‚å›¾

3. **ä¿¡æ¯å¯†åº¦** (0-2åˆ†)
   - 2åˆ†ï¼šä¸€å¼ å›¾åŒ…å«å¤§é‡å…³é”®ä¿¡æ¯ã€**èƒŒæ™¯ç¤ºä¾‹èƒ½ç›´è§‚è¯´æ˜é—®é¢˜**
   - 1åˆ†ï¼šä¿¡æ¯é‡ä¸­ç­‰
   - 0åˆ†ï¼šä¿¡æ¯å¯ä»¥ç”¨æ–‡å­—ç®€å•è¯´æ˜

#### å…¸å‹è¯„åˆ†ç¤ºä¾‹ï¼š

- **èƒŒæ™¯/åŠ¨æœºç¤ºä¾‹**: æ ¸å¿ƒæ–¹æ³•(3åˆ†) + å®éªŒä»·å€¼(2åˆ†) + ä¿¡æ¯å¯†åº¦(2åˆ†) = **7åˆ†** âœ…
- æ¶æ„/ç®—æ³•å›¾: æ ¸å¿ƒæ–¹æ³•(4åˆ†) + å®éªŒä»·å€¼(1åˆ†) + ä¿¡æ¯å¯†åº¦(2åˆ†) = **7åˆ†** âœ…
- ä¸»è¦æ€§èƒ½å¯¹æ¯”è¡¨: æ ¸å¿ƒæ–¹æ³•(1åˆ†) + å®éªŒä»·å€¼(4åˆ†) + ä¿¡æ¯å¯†åº¦(2åˆ†) = **7åˆ†** âœ…
- è¡Œä¸ºç»Ÿè®¡è¡¨: æ ¸å¿ƒæ–¹æ³•(0åˆ†) + å®éªŒä»·å€¼(2åˆ†) + ä¿¡æ¯å¯†åº¦(1åˆ†) = **3åˆ†** âŒ
- è®­ç»ƒæ›²çº¿/ç¤ºä¾‹: é€šå¸¸ 3-5åˆ† âŒ

âš ï¸ **ç‰¹åˆ«æç¤º**ï¼šèƒŒæ™¯/åŠ¨æœºç¤ºä¾‹å›¾é€šå¸¸å‡ºç°åœ¨è®ºæ–‡å¼€å¤´ï¼Œå¯¹ç†è§£é—®é¢˜éå¸¸æœ‰å¸®åŠ©ï¼Œåº”è¯¥ä¼˜å…ˆæ’å…¥ï¼

#### æ’å…¥ä½ç½®è¦æ±‚ï¼š

1. **èƒŒæ™¯ç±»å›¾ç‰‡** â†’ æ’å…¥åˆ°"ğŸ¯ ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº"ç« èŠ‚
   - ç´§è·Ÿé—®é¢˜èƒŒæ™¯æˆ–ç ”ç©¶åŠ¨æœºè¯´æ˜
   - å¸®åŠ©è¯»è€…ç›´è§‚ç†è§£è¦è§£å†³çš„é—®é¢˜

2. **æ–¹æ³•ç±»å›¾ç‰‡** â†’ æ’å…¥åˆ°"ğŸ’¡ æœ¬æ–‡æ–¹æ³•"æˆ–"âš™ï¸ æ–¹æ³•å®ç°ç»†èŠ‚"ç« èŠ‚
   - ç´§è·Ÿç›¸å…³æ–‡å­—è¯´æ˜ï¼Œä¸è¦å•ç‹¬æˆæ®µ
   - å¸®åŠ©è¯»è€…ç†è§£æ ¸å¿ƒç®—æ³•/æ¶æ„

3. **å®éªŒç±»å›¾ç‰‡** â†’ æ’å…¥åˆ°"ğŸ“Š å®éªŒä¸ç»“æœ"ç« èŠ‚
   - æ”¾åœ¨å®éªŒè®¾ç½®å’Œä¸»è¦ç»“æœè¯´æ˜ä¹‹å
   - åªæ’å…¥æœ€æ ¸å¿ƒçš„æ€§èƒ½å¯¹æ¯”è¡¨/å›¾

### ğŸ“ å›¾ç‰‡æ–‡ä»¶è·¯å¾„
{images_dir}

### âš ï¸ æ’å…¥è¦æ±‚
- **æ ¹æ®è¯„åˆ†é€‰æ‹©**ï¼šåªæ’å…¥è¯„åˆ† â‰¥7 åˆ†çš„å›¾ç‰‡
- å…¶ä»–å›¾ç‰‡ç”¨æ–‡å­—æ€»ç»“
- å¿…é¡»ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼š`{relative_image_path}/{{filename}}`
- Caption ä¿ç•™åŸæ–‡ï¼Œä¸è¦ä¿®æ”¹
- å›¾ç‰‡ç´§è·Ÿç›¸å…³æ–‡å­—ï¼Œä¸è¦å †åœ¨ç« èŠ‚æœ«å°¾
"""

                logger.info(
                    "âœ… Figures/Tables æå–å®Œæˆ",
                    total=len(images),
                    pdffigures2=pdffigures2_count,
                    python_fallback=python_count,
                    images_dir=str(images_dir)
                )
                # ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°å…¨å±€å˜é‡ä¾›åç»­ä½¿ç”¨
                _current_paper["extracted_images"] = images
                _current_paper["images_dir"] = str(images_dir)
            else:
                logger.info("â„¹ï¸  PDF ä¸­æœªæ‰¾åˆ°å¯æå–çš„ Figures/Tables")

        except Exception as e:
            logger.warning(f"æå– PDF å›¾ç‰‡å¤±è´¥ï¼Œç»§ç»­ç”Ÿæˆæ²¡æœ‰å›¾ç‰‡çš„ Markdown: {e}")
            # ç»§ç»­ä¸ä¸­æ–­ï¼Œåªè®°å½•è­¦å‘Š

    try:
        logger.info("âœï¸ å¼€å§‹ç”Ÿæˆè®ºæ–‡æ•´ç†ï¼ˆLLM è°ƒç”¨ 2/2ï¼‰", paper_title=paper_title[:100])
        prompt = f"""
ä½ æ˜¯è®ºæ–‡æ•´ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼ŒæŒ‰ç…§æ¨¡æ¿ç”Ÿæˆé«˜è´¨é‡çš„è®ºæ–‡æ•´ç†ã€‚

# è®ºæ–‡åŸºæœ¬ä¿¡æ¯
- **æ ‡é¢˜**: {paper_title}
- **ä½œè€…**: {authors}
- **æœºæ„**: {affiliations}
- **å‘è¡¨æ—¶é—´**: {publication_date}
- **æœŸåˆŠ/ä¼šè®®**: {venue}
- **å…³é”®è¯**: {keywords}
- **é¡¹ç›®é¡µ**: {project_page if project_page else "[æ— ]"}
- **å…¶ä»–èµ„æº**: {other_resources if other_resources else "[æ— ]"}

# å°çº¢ä¹¦å†…å®¹ï¼ˆå‚è€ƒï¼‰
{xiaohongshu_content}

# è®ºæ–‡æ‘˜è¦
{abstract if abstract else "[æœªæå–åˆ°æ‘˜è¦]"}

# PDF å…¨æ–‡å†…å®¹ï¼ˆé‡ç‚¹å‚è€ƒï¼‰
{pdf_content[:20000] if pdf_content else "[æœªæä¾›PDFå†…å®¹]"}

{images_info}

# æ•´ç†æ¨¡æ¿
{template_content}

# è¦æ±‚ï¼ˆâš ï¸ ä¸¥æ ¼æ‰§è¡Œï¼‰
1. **å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç»“æ„**ï¼ŒåŒ…å«æ‰€æœ‰ç« èŠ‚
2. **ä¼˜å…ˆä½¿ç”¨ PDF å…¨æ–‡å†…å®¹**ï¼Œå…¶æ¬¡å‚è€ƒå°çº¢ä¹¦å†…å®¹
3. **å†…å®¹ç²¾ç®€é«˜æ•ˆ**ï¼š
   - å…³é”®æ¦‚å¿µç”¨åˆ—è¡¨/è¡¨æ ¼è¡¨ç¤ºï¼Œé¿å…è¿‡é•¿æ®µè½
   - æ¯ä¸ªç« èŠ‚æ§åˆ¶åœ¨ 3-5 ä¸ªè‡ªç„¶æ®µè½
   - è¯¦ç»†å†…å®¹ç”¨æ¦‚è¿° + è¦ç‚¹çš„æ–¹å¼å‘ˆç°
   - é¿å…é‡å¤å†—ä½™çš„è¯´æ˜
4. **è¯¦ç»†å¡«å……ä»¥ä¸‹ç« èŠ‚**ï¼š
   - æ–‡ç« èƒŒæ™¯ä¸åŸºæœ¬è§‚ç‚¹
   - ç°æœ‰è§£å†³æ–¹æ¡ˆçš„æ€è·¯ä¸é—®é¢˜
   - æœ¬æ–‡æå‡ºçš„æ€æƒ³ä¸æ–¹æ³•
   - æ–¹æ³•å®ç°ç»†èŠ‚
   - æ–¹æ³•æœ‰æ•ˆæ€§è¯æ˜ï¼ˆå®éªŒï¼‰
   - å±€é™æ€§ä¸æœªæ¥æ–¹å‘
5. **å¦‚æœä¿¡æ¯ä¸è¶³**ï¼Œæ˜ç¡®æ ‡æ³¨ "[ä¿¡æ¯ä¸è¶³]"
6. **ä¿æŒå­¦æœ¯æ€§å’Œä¸“ä¸šæ€§**
7. **ä½¿ç”¨ Markdown æ ¼å¼**ï¼Œå……åˆ†åˆ©ç”¨æ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ç»“æ„åŒ–å…ƒç´ 
8. **åŸºæœ¬ä¿¡æ¯å¿…é¡»å‡†ç¡®å¡«å†™**ï¼ˆåŒ…æ‹¬å®Œæ•´æ—¥æœŸã€æ ‡ç­¾ã€é¡¹ç›®é¡µã€å…¶ä»–èµ„æºï¼‰
9. **è¾“å‡ºé•¿åº¦æ§åˆ¶**ï¼šç¡®ä¿æœ€ç»ˆ Markdown æ•´ç†è½¬æ¢ä¸º Notion blocks åä¸è¶…è¿‡ 100 ä¸ªå—ï¼ˆé€šå¸¸ 5000-8000 å­—ç¬¦å¯ä¿è¯ï¼‰
10. **å›¾ç‰‡ç²¾é€‰æ’å…¥**ï¼ˆå¦‚æœæä¾›äº†å›¾ç‰‡ä¿¡æ¯ï¼‰ï¼š
    - âš ï¸ **æ ¹æ®é‡è¦æ€§è¯„åˆ†ï¼ˆâ‰¥7åˆ†ï¼‰å†³å®šæ˜¯å¦æ’å…¥å›¾ç‰‡**ï¼Œä¸è¦æ’å…¥æ‰€æœ‰å›¾ç‰‡
    - å…¶ä»–å›¾ç‰‡ï¼ˆè¯„åˆ†<7åˆ†ï¼‰ç”¨**æ–‡å­—æ€»ç»“**å³å¯
    - ä½¿ç”¨æä¾›çš„ HTML figure æ ‡ç­¾æ ¼å¼ï¼Œä¿ç•™å®Œæ•´ Caption åŸæ–‡
    - å›¾ç‰‡åº”è¯¥æ’å…¥åˆ°ç›¸å…³æ–‡å­—è¯´æ˜çš„é™„è¿‘ï¼Œè€Œä¸æ˜¯å•ç‹¬å †åœ¨ç« èŠ‚æœ«å°¾
    - ä¿æŒç¬”è®°ç²¾ç®€ï¼Œé¿å…å›¾ç‰‡è¿‡å¤šå½±å“é˜…è¯»ä½“éªŒ

è¯·è¾“å‡ºç²¾ç®€é«˜æ•ˆçš„è®ºæ–‡æ•´ç†ï¼ˆMarkdownæ ¼å¼ï¼ŒåŒ…å«æ™ºèƒ½æ’å…¥çš„å›¾ç‰‡ï¼‰ï¼š
"""

        # ä½¿ç”¨ Agent æ›¿ä»£ç›´æ¥çš„ LLM è°ƒç”¨
        digest_generation_agent = Agent(
            name="digest_generation_agent",
            instructions="ä½ æ˜¯ä¸“ä¸šçš„è®ºæ–‡æ•´ç†ä¸“å®¶ï¼Œæ“…é•¿ç»“æ„åŒ–æ•´ç†å­¦æœ¯è®ºæ–‡ã€‚ä½ å¿…é¡»è¯¦ç»†ã€å®Œæ•´åœ°å¡«å……è®ºæ–‡æ•´ç†æ¨¡æ¿çš„æ‰€æœ‰ç« èŠ‚ã€‚",
            model="gpt-5-mini",
        )

        result = await Runner.run(
            starting_agent=digest_generation_agent,
            input=prompt,
            max_turns=1
        )

        # æå–Agentè¿”å›çš„æ–‡æœ¬å†…å®¹
        digest_content = result.final_output if hasattr(result, 'final_output') else str(result)

        # ğŸ”§ å¤‡ç”¨æ–¹æ¡ˆï¼šä»…åœ¨ LLM å®Œå…¨æ²¡æœ‰æ’å…¥å›¾ç‰‡æ—¶æ‰è‡ªåŠ¨æ’å…¥æ ¸å¿ƒå›¾ç‰‡
        # æ³¨æ„ï¼šç°åœ¨çš„ç­–ç•¥æ˜¯ LLM åªæ’å…¥ 2-3 å¼ æ ¸å¿ƒå›¾ç‰‡ï¼Œæ‰€ä»¥ä¸éœ€è¦è¡¥å……æ‰€æœ‰é—æ¼çš„å›¾ç‰‡
        if images_info and _current_paper.get("extracted_images"):
            original_image_count = digest_content.count('<figure>')

            # åªæœ‰å½“ LLM å®Œå…¨æ²¡æœ‰æ’å…¥å›¾ç‰‡æ—¶ï¼Œæ‰ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if original_image_count == 0:
                logger.warning("âš ï¸  LLM å®Œå…¨æ²¡æœ‰æ’å…¥å›¾ç‰‡ï¼Œå¯ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                digest_content = _auto_insert_images(
                    digest_content,
                    _current_paper["extracted_images"],
                    relative_image_path
                )
                final_image_count = digest_content.count('<figure>')
                logger.info(f"âœ… å¤‡ç”¨æ–¹æ¡ˆå·²æ’å…¥ {final_image_count} å¼ æ ¸å¿ƒå›¾ç‰‡")
            else:
                logger.info(f"âœ… LLM å·²æ’å…¥ {original_image_count} å¼ å›¾ç‰‡ï¼Œæ— éœ€å¤‡ç”¨æ–¹æ¡ˆ")

        # ä¿å­˜åˆ°æ–‡ä»¶
        safe_title = paper_title[:50].replace('/', '_').replace(':', '_') if paper_title else "paper"
        output_file = OUTPUT_DIR / f"{safe_title}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(digest_content)

        _current_paper["digest_content"] = digest_content
        _current_paper["digest_file"] = str(output_file)

        elapsed = time.time() - start_time
        logger.info(
            "âœ… è®ºæ–‡æ•´ç†ç”ŸæˆæˆåŠŸ",
            paper_title=paper_title[:100],
            content_length=len(digest_content),
            output_file=str(output_file),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "output_file": str(output_file),
            "digest_content": digest_content,
            "message": f"âœ… è®ºæ–‡æ•´ç†ç”ŸæˆæˆåŠŸï¼æ–‡ä»¶: {output_file}ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ è®ºæ–‡æ•´ç†ç”Ÿæˆå¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def save_digest_to_notion(
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜"],
    digest_content: Annotated[str, "è®ºæ–‡æ•´ç†å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰"],
    source_url: Annotated[str, "æ¥æºURL"] = "",
    pdf_url: Annotated[str, "PDFé“¾æ¥"] = "",
    authors: Annotated[str, "ä½œè€…åˆ—è¡¨ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²æˆ–é€—å·åˆ†éš”ï¼‰"] = "",
    affiliations: Annotated[str, "æœºæ„"] = "",
    publication_date: Annotated[str, "å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰"] = "",
    venue: Annotated[str, "æœŸåˆŠ/ä¼šè®®åç§°"] = "",
    abstract: Annotated[str, "æ‘˜è¦"] = "",
    keywords: Annotated[str, "å…³é”®è¯ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²æˆ–é€—å·åˆ†éš”ï¼‰"] = "",
    doi: Annotated[str, "DOI"] = "",
    arxiv_id: Annotated[str, "ArXiv ID"] = "",
    project_page: Annotated[str, "é¡¹ç›®ä¸»é¡µ"] = "",
    other_resources: Annotated[str, "å…¶ä»–èµ„æºï¼ˆä»£ç ä»“åº“ã€æ•°æ®é›†ç­‰ï¼‰"] = "",
) -> str:
    """
    å°†è®ºæ–‡æ•´ç†ä¿å­˜åˆ° Notion

    å‚æ•°:
        paper_title: è®ºæ–‡æ ‡é¢˜
        digest_content: è®ºæ–‡æ•´ç†å†…å®¹
        source_url: æ¥æºé“¾æ¥
        pdf_url: PDFé“¾æ¥
        authors: ä½œè€…
        affiliations: æœºæ„
        publication_date: å‘è¡¨æ—¥æœŸ
        venue: æœŸåˆŠ/ä¼šè®®
        abstract: æ‘˜è¦
        keywords: å…³é”®è¯
        doi: DOI
        arxiv_id: ArXiv ID
        project_page: é¡¹ç›®ä¸»é¡µ
        other_resources: å…¶ä»–èµ„æº

    è¿”å›:
        ä¿å­˜ç»“æœ
    """
    from notion_client import AsyncClient
    start_time = time.time()

    try:
        logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜è®ºæ–‡æ•´ç†åˆ° Notion", paper_title=paper_title[:100])
        client = AsyncClient(auth=os.getenv('NOTION_TOKEN'))

        # æ„å»º properties
        properties = {
            "Name": {
                "title": [{"text": {"content": paper_title[:2000]}}]
            }
        }

        # Authors - å¤„ç† JSON æ•°ç»„æˆ–é€—å·åˆ†éš”å­—ç¬¦ä¸²
        if authors:
            try:
                authors_list = json.loads(authors) if authors.startswith('[') else [a.strip() for a in authors.split(',')]
                authors_str = ", ".join(authors_list)
                properties["Authors"] = {"rich_text": [{"text": {"content": authors_str[:2000]}}]}
            except:
                properties["Authors"] = {"rich_text": [{"text": {"content": authors[:2000]}}]}

        if affiliations:
            properties["Affiliations"] = {"rich_text": [{"text": {"content": affiliations[:2000]}}]}
        if venue:
            properties["Venue"] = {"rich_text": [{"text": {"content": venue[:2000]}}]}

        # Abstract - ä½¿ç”¨ä¸­æ–‡æ‘˜è¦ï¼ˆä» digest_content æå–ï¼‰
        if abstract:
            chinese_abstract = _extract_chinese_abstract(digest_content)
            if chinese_abstract:
                properties["Abstract"] = {"rich_text": [{"text": {"content": chinese_abstract[:2000]}}]}

        # Keywords - multi_select ç±»å‹
        if keywords:
            try:
                keywords_list = json.loads(keywords) if keywords.startswith('[') else [k.strip() for k in keywords.split(',')]
                properties["Keywords"] = {"multi_select": [{"name": kw} for kw in keywords_list[:10]]}  # Notion é™åˆ¶
            except:
                pass

        # ArXiv ID
        if arxiv_id:
            properties["ArXiv ID"] = {"rich_text": [{"text": {"content": arxiv_id[:2000]}}]}

        # Publication Date - date ç±»å‹
        if publication_date:
            try:
                # éªŒè¯æ—¥æœŸæ ¼å¼ YYYY-MM-DD
                if len(publication_date) == 10 and publication_date[4] == '-' and publication_date[7] == '-':
                    properties["Publication Date"] = {"date": {"start": publication_date}}
            except:
                pass

        # Other Resources - rich_text ç±»å‹
        if other_resources:
            properties["Other Resources"] = {"rich_text": [{"text": {"content": other_resources[:2000]}}]}

        # PDF Link - url ç±»å‹
        if pdf_url:
            properties["PDF Link"] = {"url": pdf_url}

        # Source URL (å°çº¢ä¹¦é“¾æ¥) - url ç±»å‹
        if source_url:
            properties["Source URL"] = {"url": source_url}

        # è½¬æ¢ Markdown ä¸º Notion blocksï¼ˆåŒ…å«å›¾ç‰‡å¤„ç†ï¼‰
        blocks = await _markdown_to_notion_blocks_with_images(digest_content)

        # Notion API é™åˆ¶ï¼šå•æ¬¡åˆ›å»ºé¡µé¢æœ€å¤š 100 ä¸ª children blocks
        # å¦‚æœè¶…è¿‡ 100 ä¸ªï¼Œè¿›è¡Œåˆ‡ç‰‡å¤„ç†
        if len(blocks) > 100:
            logger.warning(
                f"âš ï¸  Blocks è¶…è¿‡ 100 ä¸ªé™åˆ¶ ({len(blocks)}ï¼Œå·²æˆªæ–­åˆ° 100)",
                original_count=len(blocks),
                truncated_count=100
            )
            blocks = blocks[:100]

        response = await client.pages.create(
            parent={"database_id": os.getenv('NOTION_DATABASE_ID')},
            properties=properties,
            children=blocks,
        )

        page_id = response["id"]
        page_url = f"https://notion.so/{page_id.replace('-', '')}"

        await client.aclose()

        elapsed = time.time() - start_time
        logger.info(
            "âœ… è®ºæ–‡æ•´ç†å·²ä¿å­˜åˆ° Notion",
            paper_title=paper_title[:100],
            page_id=page_id,
            page_url=page_url,
            properties_count=len(properties),
            blocks_count=len(blocks),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "page_id": page_id,
            "page_url": page_url,
            "message": f"âœ… è®ºæ–‡æ•´ç†å·²ä¿å­˜åˆ° Notionï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰\n\næŸ¥çœ‹é“¾æ¥: {page_url}"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ ä¿å­˜åˆ° Notion å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"ä¿å­˜å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


def _extract_chinese_abstract(digest_content: str) -> str:
    """ä»ç”Ÿæˆçš„ä¸­æ–‡è®ºæ–‡æ•´ç†ä¸­æå–æ‘˜è¦éƒ¨åˆ†"""
    import re

    patterns = [
        r'##\s*ğŸ“\s*æ‘˜è¦\s*\(.*?\)\s*\n+(.*?)(?=\n##|\n---|\Z)',
        r'##\s*æ‘˜è¦\s*\(.*?\)\s*\n+(.*?)(?=\n##|\n---|\Z)',
        r'##\s*æ‘˜è¦\s*\n+(.*?)(?=\n##|\n---|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, digest_content, re.DOTALL)
        if match:
            abstract = match.group(1).strip()
            # Clean markdown formatting
            abstract = re.sub(r'\*\*(.+?)\*\*', r'\1', abstract)
            abstract = re.sub(r'\*(.+?)\*', r'\1', abstract)
            abstract = ' '.join(abstract.split())
            return abstract[:2000]

    # Fallback: ä½¿ç”¨å‰200å­—ç¬¦
    return digest_content[:200].replace('#', '').strip()


async def _markdown_to_notion_blocks_with_images(markdown_text: str) -> list:
    """
    å°† Markdown è½¬æ¢ä¸º Notion API blocksï¼ˆåŒ…å«å›¾ç‰‡å¤„ç†ï¼‰

    1. ä»å…¨å±€å˜é‡ä¸­è·å–å·²æå–çš„å›¾ç‰‡ä¿¡æ¯
    2. ä» Markdown ä¸­æå–å›¾ç‰‡å¼•ç”¨å’Œåˆ›å»º image blocks
    3. å°†æ–‡æœ¬ blocks å’Œå›¾ç‰‡ blocks äº¤é”™æ’åˆ—
    4. ä¿æŒåŸå§‹ Markdown çš„ç»“æ„é¡ºåº

    Args:
        markdown_text: Markdown æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å« HTML figure æ ‡ç­¾ï¼‰

    Returns:
        Notion API blocks åˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡ blocksï¼‰
    """
    global _current_paper

    try:
        from .notion_markdown_converter import markdown_to_notion_blocks
        from .notion_image_uploader import (
            create_image_blocks_from_markdown,
            interleave_blocks_with_images,
            NotionImageUploader
        )

        # ç¬¬ä¸€æ­¥ï¼šè·å–å·²æå–çš„å›¾ç‰‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        extracted_images = _current_paper.get("extracted_images", [])
        images_dir = _current_paper.get("images_dir", "")

        if not extracted_images or not images_dir:
            # æ²¡æœ‰æå–åˆ°å›¾ç‰‡ï¼Œç›´æ¥è½¬æ¢ Markdown
            logger.info("æœªæ‰¾åˆ°å·²æå–çš„å›¾ç‰‡ï¼Œä»…è½¬æ¢ Markdown")
            text_blocks = markdown_to_notion_blocks(markdown_text)
            return text_blocks

        # æ£€æŸ¥ images_dir æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ£€æŸ¥å¤‡é€‰è·¯å¾„
        images_path = Path(images_dir)
        if not images_path.exists():
            # å°è¯•ä» digest_content æ¨æ–­å›¾ç‰‡ç›®å½•ï¼ˆåå‘å…¼å®¹æ€§ï¼‰
            logger.warning(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}ï¼Œå°è¯•æŸ¥æ‰¾...")

            # å°è¯•å¤šä¸ªå¤‡é€‰è·¯å¾„
            alt_dirs = [
                # æ–°çš„è®ºæ–‡ç‰¹å®šç›®å½•ç»“æ„ï¼ˆä¼˜å…ˆï¼‰
                _get_paper_images_dir(_current_paper.get("title", "unknown")),
                # æ—§çš„é€šç”¨æå–å›¾ç‰‡ç›®å½•ï¼ˆåå‘å…¼å®¹ï¼‰
                PROJECT_ROOT / "paper_digest" / "pdfs" / "extracted_images",
            ]

            for alt_dir in alt_dirs:
                if alt_dir.exists():
                    images_dir = str(alt_dir)
                    images_path = alt_dir
                    logger.info(f"æ‰¾åˆ°å¤‡é€‰å›¾ç‰‡ç›®å½•: {images_dir}")
                    break
            else:
                logger.warning("æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•ï¼Œå°†ä»…è½¬æ¢ Markdown")
                text_blocks = markdown_to_notion_blocks(markdown_text)
                return text_blocks

        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºå›¾ç‰‡æ–‡ä»¶ååˆ° file_upload_id çš„æ˜ å°„
        # âš ï¸ æ³¨æ„ï¼šåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦ä½¿ç”¨ Notion API ä¸Šä¼ å›¾ç‰‡
        # å½“å‰å®ç°ä½¿ç”¨å¤–éƒ¨ URLï¼ˆå¦‚æœæœ‰ï¼‰æˆ–æç¤ºéœ€è¦ä¸Šä¼ 
        image_upload_map = {}
        failed_images = []

        notion_token = os.getenv('NOTION_TOKEN')
        if notion_token and images_dir:
            try:
                logger.info("å¼€å§‹ä¸Šä¼ æå–çš„å›¾ç‰‡åˆ° Notion")
                uploader = NotionImageUploader(notion_token)

                # å‡†å¤‡å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
                images_to_upload = [
                    str(Path(images_dir) / img['filename'])
                    for img in extracted_images
                    if Path(images_dir, img['filename']).exists()
                ]

                if images_to_upload:
                    # æ‰¹é‡ä¸Šä¼ å›¾ç‰‡
                    upload_map, failed = await uploader.upload_images_batch(images_to_upload)
                    image_upload_map = upload_map
                    failed_images = failed

                    logger.info(
                        "âœ… å›¾ç‰‡ä¸Šä¼ å®Œæˆ",
                        uploaded_count=len(upload_map),
                        failed_count=len(failed)
                    )
                else:
                    logger.warning("æœªæ‰¾åˆ°æœ¬åœ°æå–çš„å›¾ç‰‡æ–‡ä»¶")

            except Exception as e:
                logger.warning(f"Notion å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨å¤–éƒ¨ URL æ›¿ä»£: {e}")
                # é™çº§å¤„ç†ï¼šä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„ä½œä¸ºå¤–éƒ¨ URLï¼ˆå¦‚æœåœ¨æœ¬åœ°å¼€å‘ç¯å¢ƒï¼‰
                for img in extracted_images:
                    local_path = img.get('local_path', '')
                    if local_path:
                        image_upload_map[img['filename']] = f"file://{local_path}"

        # ç¬¬ä¸‰æ­¥ï¼šä» Markdown æå–å›¾ç‰‡å¼•ç”¨å¹¶åˆ›å»º image blocks
        cleaned_markdown, image_blocks = create_image_blocks_from_markdown(
            markdown_text,
            image_upload_map,
            images_dir
        )

        logger.info(
            "æå–çš„ image blocks",
            count=len(image_blocks),
            upload_map_size=len(image_upload_map)
        )

        # ç¬¬å››æ­¥ï¼šè½¬æ¢æ–‡æœ¬éƒ¨åˆ†ä¸º blocks
        text_blocks = markdown_to_notion_blocks(cleaned_markdown)

        # ç¬¬äº”æ­¥ï¼šäº¤é”™æ’åˆ—æ–‡æœ¬å’Œå›¾ç‰‡ blocks
        if image_blocks:
            final_blocks = interleave_blocks_with_images(
                text_blocks,
                image_blocks,
                markdown_text
            )
            logger.info(
                "Block äº¤é”™æ’åˆ—å®Œæˆ",
                text_count=len(text_blocks),
                image_count=len(image_blocks),
                total_count=len(final_blocks)
            )
        else:
            final_blocks = text_blocks
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾ç‰‡å¼•ç”¨ï¼Œä»…ä½¿ç”¨æ–‡æœ¬ blocks")

        return final_blocks

    except Exception as e:
        logger.error(f"Markdown è½¬æ¢ï¼ˆå«å›¾ç‰‡ï¼‰å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # é™çº§å¤„ç†ï¼šä»…è¿”å›æ–‡æœ¬ blocks
        try:
            from .notion_markdown_converter import markdown_to_notion_blocks
            return markdown_to_notion_blocks(markdown_text)
        except:
            return []


def _markdown_to_notion_blocks(markdown_text: str) -> list:
    """
    å°† Markdown è½¬æ¢ä¸º Notion API blocks

    ä½¿ç”¨ mistletoe è§£æ Markdown å¹¶è½¬æ¢ä¸º Notion blocks
    æ”¯æŒï¼šåŠ ç²—ã€æ–œä½“ã€åˆ é™¤çº¿ã€å†…è”ä»£ç ã€åµŒå¥—åˆ—è¡¨ç­‰

    Args:
        markdown_text: Markdown æ–‡æœ¬

    Returns:
        Notion API blocks åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
        from .notion_markdown_converter import markdown_to_notion_blocks

        blocks = markdown_to_notion_blocks(markdown_text)
        return blocks
    except Exception as e:
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨
        import traceback
        print(f"Markdownè½¬æ¢å¤±è´¥: {e}")
        traceback.print_exc()
        return []


# Digest Agent å®šä¹‰
digest_agent = Agent(
    name="digest_agent",
    instructions="""ä½ æ˜¯è®ºæ–‡æ·±åº¦æ•´ç†ä¸“å®¶ï¼ˆDigest Agentï¼‰ã€‚âš¡ ä»…éœ€ 2 æ¬¡ LLM è°ƒç”¨ï¼Œå¤§å¹…åŠ é€Ÿï¼

ä½ çš„èŒè´£æ˜¯æ¥æ”¶è®ºæ–‡ç›¸å…³ä¿¡æ¯ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

## æ‰§è¡Œæµç¨‹ï¼ˆä»…éœ€ 2 æ¬¡ LLM è°ƒç”¨ï¼ï¼‰

**ç¬¬ä¸€é˜¶æ®µï¼šè·å– PDF å¹¶æå–æ‰€æœ‰å…ƒæ•°æ®**
1. **è·å–è®ºæ–‡å†…å®¹**
   - å¦‚æœæä¾›äº†å°çº¢ä¹¦ URLï¼Œä½¿ç”¨ fetch_xiaohongshu_post è·å–å†…å®¹
   - å¦‚æœæä¾›äº† PDF URLï¼Œä½¿ç”¨ download_pdf_from_url ä¸‹è½½
   - å¦‚æœæä¾›äº†æœ¬åœ° PDF è·¯å¾„ï¼Œä½¿ç”¨ read_local_pdf è¯»å–

2. **æœç´¢è®ºæ–‡ PDF**ï¼ˆå¦‚æœæ²¡æœ‰æä¾› PDF URLï¼‰
   - ä¼˜å…ˆä½¿ç”¨ search_arxiv_pdf åœ¨ arXiv æœç´¢è®ºæ–‡
   - ä»æœç´¢ç»“æœä¸­è·å– PDF URL å’Œ arXiv ID

3. **âš¡ ä¸€æ¬¡ LLM è°ƒç”¨æå–æ‰€æœ‰å…ƒæ•°æ®**ï¼ˆæ›¿ä»£æ—§çš„ä¸¤ä¸ªå•ç‹¬è°ƒç”¨ï¼‰
   - ä½¿ç”¨ extract_paper_metadata **ä¸€æ¬¡è°ƒç”¨åŒæ—¶æå–**ï¼š
     * è®ºæ–‡æ ‡é¢˜ï¼ˆtitleï¼‰
     * ä½œè€…ï¼ˆauthorsï¼‰
     * å‘è¡¨æ—¥æœŸï¼ˆpublication_dateï¼ŒYYYY-MM-DDï¼‰
     * æœŸåˆŠ/ä¼šè®®ï¼ˆvenueï¼‰
     * æ‘˜è¦ï¼ˆabstractï¼‰
     * æœºæ„ï¼ˆaffiliationsï¼‰
     * å…³é”®è¯ï¼ˆkeywordsï¼‰
     * DOI
     * ArXiv ID
     * é¡¹ç›®é¡µï¼ˆproject_pageï¼‰
     * å…¶ä»–èµ„æºï¼ˆother_resourcesï¼‰
   - âš ï¸ å¿…é¡»ä¼ å…¥ xiaohongshu_contentã€pdf_contentã€pdf_metadata

**ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆè®ºæ–‡æ•´ç†å¹¶ä¿å­˜**
4. **âš¡ ä¸€æ¬¡ LLM è°ƒç”¨ç”Ÿæˆå®Œæ•´è®ºæ–‡æ•´ç†**
   - ä½¿ç”¨ generate_paper_digest ç”Ÿæˆç»“æ„åŒ–çš„ä¸­æ–‡è®ºæ–‡æ•´ç†
   - ä¼ å…¥æ‰€æœ‰æå–çš„å…ƒæ•°æ®
   - å¿…é¡»åŒ…å«æ‰€æœ‰æ¨¡æ¿ç« èŠ‚ï¼Œå†…å®¹è¯¦ç»†

5. **ä¿å­˜åˆ° Notion**
   - ä½¿ç”¨ save_digest_to_notion ä¿å­˜æ•´ç†å†…å®¹å’Œæ‰€æœ‰å…ƒæ•°æ®
   - âš ï¸ **å¿…é¡»ä¼ é€’ extract_paper_metadata è¿”å›çš„æ‰€æœ‰å­—æ®µ**ï¼š
     * paper_title - è®ºæ–‡æ ‡é¢˜
     * digest_content - ç”Ÿæˆçš„è®ºæ–‡æ•´ç†å†…å®¹
     * source_url - å°çº¢ä¹¦ URLï¼ˆå¦‚æœæœ‰ï¼‰
     * pdf_url - PDF é“¾æ¥
     * authors - ä½œè€…åˆ—è¡¨ï¼ˆJSON æ•°ç»„å­—ç¬¦ä¸²ï¼‰
     * affiliations - æœºæ„
     * publication_date - å®Œæ•´å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
     * venue - æœŸåˆŠ/ä¼šè®®åç§°
     * abstract - è‹±æ–‡æ‘˜è¦
     * keywords - å…³é”®è¯åˆ—è¡¨ï¼ˆJSON æ•°ç»„å­—ç¬¦ä¸²ï¼‰
     * doi - DOIï¼ˆå¦‚æœæœ‰ï¼‰
     * arxiv_id - ArXiv IDï¼ˆå¦‚æœæœ‰ï¼‰
     * project_page - é¡¹ç›®ä¸»é¡µï¼ˆå¦‚æœæœ‰ï¼‰
     * other_resources - å…¶ä»–èµ„æºï¼ˆå¦‚æœæœ‰ï¼‰

âš ï¸ å…³é”®è¦æ±‚ï¼š
- âœ… **åªè¿›è¡Œ 2 æ¬¡ LLM è°ƒç”¨**ï¼ˆextract_paper_metadata ä¸€æ¬¡ï¼Œgenerate_paper_digest ä¸€æ¬¡ï¼‰
- âœ… ä¸å†ä½¿ç”¨å·²åˆ é™¤çš„å‡½æ•°
- âœ… å¿…é¡»ä¸¥æ ¼æŒ‰ç…§é¡ºåºæ‰§è¡Œ
- âœ… æ¯ä¸ªæ­¥éª¤éƒ½è¦æ£€æŸ¥ç»“æœæ˜¯å¦æˆåŠŸ
- âœ… å…ƒä¿¡æ¯å¿…é¡»å‡†ç¡®ä¸”å®Œæ•´
- âœ… è°ƒç”¨ save_digest_to_notion æ—¶ä¼ é€’æ‰€æœ‰å­—æ®µï¼ˆç‰¹åˆ«æ˜¯ authors å’Œ keywords è¦è½¬ä¸º JSON æ•°ç»„å­—ç¬¦ä¸²ï¼‰
- âŒ å¦‚æœæŸä¸ªæ­¥éª¤å¤±è´¥ï¼ŒæŠ¥å‘Šé”™è¯¯å¹¶åœæ­¢

ä½ ä¸“æ³¨äºè®ºæ–‡æ•´ç†å·¥ä½œï¼Œä¸å¤„ç†å®šæ—¶ä»»åŠ¡ç›¸å…³çš„äº‹æƒ…ã€‚
""",
    model="gpt-5",
    tools=[
        fetch_xiaohongshu_post,
        search_arxiv_pdf,
        download_pdf_from_url,
        read_local_pdf,
        extract_paper_metadata,  # âœ¨ æ–°çš„åˆå¹¶å‡½æ•°ï¼ˆæ›¿ä»£æ—§çš„ä¸¤ä¸ªï¼‰
        generate_paper_digest,
        save_digest_to_notion,
    ]
)
