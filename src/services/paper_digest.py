#!/usr/bin/env python3
"""
Digest Agent Core - è®ºæ–‡æ•´ç†æ ¸å¿ƒ Agentï¼ˆä½œä¸º sub-agentï¼‰

åŠŸèƒ½ï¼š
1. æ¥æ”¶å¤šç§è¾“å…¥æºï¼ˆXHS URLã€PDF URLã€PDF æœ¬åœ°è·¯å¾„ï¼‰
2. ä¸‹è½½ PDF å¹¶è¯»å–å…¨æ–‡å†…å®¹
3. ç”Ÿæˆç»“æ„åŒ–è®ºæ–‡æ•´ç†
4. ä¿å­˜åˆ° Notion æ•°æ®åº“

ä½œä¸º sub-agent è¢«ä¸» paper_agent è°ƒç”¨
"""

import asyncio
import os
import sys
from pathlib import Path
from typing import Annotated
import json
import httpx
import fitz  # PyMuPDF
import time

from agents import Agent, function_tool
from openai import AsyncOpenAI
from ..utils.logger import get_logger

# åˆå§‹åŒ–æ—¥å¿—
logger = get_logger(__name__)

# é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent  # spec-paper-notion-agent/
DIGEST_ROOT = Path(__file__).resolve().parent  # src/services/
OUTPUT_DIR = PROJECT_ROOT / "paper_digest" / "outputs"  # ä¿æŒè¾“å‡ºåœ¨åŸä½ç½®
PDF_DIR = PROJECT_ROOT / "paper_digest" / "pdfs"  # ä¿æŒPDFåœ¨åŸä½ç½®

# ç¡®ä¿ç›®å½•å­˜åœ¨
OUTPUT_DIR.mkdir(exist_ok=True)
PDF_DIR.mkdir(exist_ok=True)

# å…¨å±€å˜é‡
_openai_client = None
_current_paper = {}


def _init_digest_globals(openai_client):
    """åˆå§‹åŒ–å…¨å±€å˜é‡"""
    global _openai_client
    _openai_client = openai_client


@function_tool
async def fetch_xiaohongshu_post(
    post_url: Annotated[str, "å°çº¢ä¹¦å¸–å­çš„å®Œæ•´URL"]
) -> str:
    """
    è·å–å°çº¢ä¹¦å¸–å­å†…å®¹

    å‚æ•°:
        post_url: å°çº¢ä¹¦å¸–å­URL

    è¿”å›:
        JSONæ ¼å¼çš„å¸–å­ä¿¡æ¯ï¼ˆåŒ…å« raw_contentï¼‰
    """
    global _current_paper
    start_time = time.time()

    # å¯¼å…¥ xiaohongshu æœåŠ¡
    from .xiaohongshu import XiaohongshuClient

    try:
        logger.info("ğŸ” å¼€å§‹è·å–å°çº¢ä¹¦å¸–å­")
        client = XiaohongshuClient(
            cookies=os.getenv("XHS_COOKIES"),
            openai_client=_openai_client  # âœ¨ ä¼ é€’ OpenAI client ç”¨äº LLM è§£æ
        )
        post = await client.fetch_post(post_url)

        _current_paper = {
            "post_id": post.post_id,
            "post_url": str(post.post_url),
            "blogger_name": post.blogger_name,
            "raw_content": post.raw_content,
        }

        elapsed = time.time() - start_time
        logger.info(
            "âœ… å°çº¢ä¹¦å¸–å­è·å–æˆåŠŸ",
            post_id=post.post_id,
            content_length=len(post.raw_content),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "post_id": post.post_id,
            "content": post.raw_content,
            "message": f"âœ… å¸–å­å†…å®¹è·å–æˆåŠŸï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ å°çº¢ä¹¦å¸–å­è·å–å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"è·å–å¸–å­å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def extract_paper_metadata(
    xiaohongshu_content: Annotated[str, "å°çº¢ä¹¦å¸–å­å†…å®¹ï¼ˆå¯é€‰ï¼‰"] = "",
    pdf_content: Annotated[str, "PDFçš„æ–‡æœ¬å†…å®¹"] = "",
    pdf_metadata: Annotated[str, "PDFçš„å…ƒæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"] = ""
) -> str:
    """
    ä» PDF å’Œå°çº¢ä¹¦å†…å®¹ä¸­æå–æ‰€æœ‰è®ºæ–‡ä¿¡æ¯ï¼ˆä¸€æ¬¡ LLM è°ƒç”¨ï¼‰

    å‚æ•°:
        xiaohongshu_content: å°çº¢ä¹¦å¸–å­å†…å®¹ï¼ˆå¯é€‰ï¼‰
        pdf_content: PDF æ–‡æœ¬å†…å®¹
        pdf_metadata: PDF å…ƒæ•°æ® (JSONæ ¼å¼)

    è¿”å›:
        JSONæ ¼å¼çš„å®Œæ•´è®ºæ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - title: è®ºæ–‡è‹±æ–‡æ ‡é¢˜
        - authors: ä½œè€…åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
        - publication_date: å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        - venue: æœŸåˆŠ/ä¼šè®®åç§°
        - abstract: æ‘˜è¦
        - affiliations: æœºæ„
        - keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
        - doi: DOI
        - arxiv_id: ArXiv ID
        - project_page: é¡¹ç›®ä¸»é¡µ
        - other_resources: å…¶ä»–èµ„æº
    """
    global _openai_client, _current_paper
    start_time = time.time()

    try:
        logger.info("ğŸ“š å¼€å§‹æå–è®ºæ–‡å…ƒæ•°æ®ï¼ˆLLM è°ƒç”¨ 1/2ï¼‰")
        prompt = f"""ä½ æ˜¯è®ºæ–‡ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å®Œæ•´çš„è®ºæ–‡ä¿¡æ¯ã€‚

# PDF å…ƒæ•°æ®
{pdf_metadata}

# PDF å†…å®¹ï¼ˆå‰5000å­—ï¼‰
{pdf_content[:5000] if pdf_content else "[æœªæä¾›]"}

# å°çº¢ä¹¦å†…å®¹ï¼ˆå‚è€ƒï¼‰
{xiaohongshu_content[:2000] if xiaohongshu_content else "[æœªæä¾›]"}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¿…é¡»è¿”å› JSON æ ¼å¼ï¼š

{{
    "title": "è®ºæ–‡è‹±æ–‡æ ‡é¢˜ï¼ˆå¿…å¡«ï¼‰",
    "authors": ["ä½œè€…1", "ä½œè€…2"],
    "publication_date": "YYYY-MM-DDï¼ˆå¦‚æœåªæœ‰å¹´ä»½ï¼Œä½¿ç”¨ YYYY-01-01ï¼‰",
    "venue": "æœŸåˆŠ/ä¼šè®®åç§°",
    "abstract": "è‹±æ–‡æ‘˜è¦",
    "affiliations": "Stanford University; MIT",
    "keywords": ["keyword1", "keyword2", "tag1"],
    "doi": "10.1234/exampleï¼ˆå¦‚æœæœ‰ï¼‰",
    "arxiv_id": "2410.xxxxxï¼ˆå¦‚æœæ˜¯ arXiv è®ºæ–‡ï¼‰",
    "project_page": "é¡¹ç›®ä¸»é¡µé“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰",
    "other_resources": "ä»£ç ä»“åº“ã€æ•°æ®é›†ç­‰ï¼ˆå¯ç”¨åˆ†å·åˆ†éš”ï¼‰"
}}

âš ï¸ è¦æ±‚ï¼š
1. title å¿…é¡»æå–ï¼Œå…¶ä»–å­—æ®µæ²¡æœ‰ä¿¡æ¯å¯ä»¥è®¾ä¸º null
2. publication_date å¿…é¡»æ˜¯å®Œæ•´çš„ YYYY-MM-DD æ ¼å¼
3. authors å’Œ keywords å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼
4. å…¶ä»–å­—æ®µå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„ï¼Œè®¾ç½®ä¸ºå¯ç”¨çš„æ ¼å¼
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œä½¿ç”¨ null å€¼
"""

        response = await _openai_client.chat.completions.create(
            model="gpt-5-mini",
            messages=[{
                "role": "system",
                "content": "ä½ æ˜¯ä¸“ä¸šçš„è®ºæ–‡ä¿¡æ¯æå–ä¸“å®¶ã€‚ä½ å¿…é¡»å‡†ç¡®å®Œæ•´åœ°æå–è®ºæ–‡çš„æ‰€æœ‰å…ƒæ•°æ®ã€‚"
            }, {
                "role": "user",
                "content": prompt
            }],
            response_format={"type": "json_object"}
        )

        extracted_info = json.loads(response.choices[0].message.content)

        # éªŒè¯å¿…å¡«å­—æ®µ
        if not extracted_info.get("title"):
            extracted_info["title"] = "Unknown Paper"

        _current_paper.update(extracted_info)

        elapsed = time.time() - start_time
        logger.info(
            "âœ… è®ºæ–‡å…ƒæ•°æ®æå–æˆåŠŸ",
            title=extracted_info.get("title", "Unknown"),
            authors_count=len(extracted_info.get("authors", [])),
            keywords_count=len(extracted_info.get("keywords", [])),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            **extracted_info,
            "message": f"âœ… è®ºæ–‡ä¿¡æ¯æå–æˆåŠŸï¼ˆæ ‡é¢˜ + å…ƒæ•°æ®ï¼‰ï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ è®ºæ–‡å…ƒæ•°æ®æå–å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"æå–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def search_arxiv_pdf(
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜"]
) -> str:
    """
    åœ¨ arXiv æœç´¢è®ºæ–‡ PDF é“¾æ¥

    å‚æ•°:
        paper_title: è®ºæ–‡æ ‡é¢˜

    è¿”å›:
        JSONæ ¼å¼çš„PDFé“¾æ¥ä¿¡æ¯
    """
    start_time = time.time()
    try:
        import urllib.parse

        logger.info("ğŸ” å¼€å§‹åœ¨ arXiv æœç´¢è®ºæ–‡", paper_title=paper_title[:100])

        # arXiv API æœç´¢
        query = urllib.parse.quote(paper_title)
        api_url = f"http://export.arxiv.org/api/query?search_query=ti:{query}&max_results=3"

        proxy = os.getenv('http_proxy')
        mounts = None
        if proxy:
            mounts = {
                "http://": httpx.AsyncHTTPTransport(proxy=proxy),
                "https://": httpx.AsyncHTTPTransport(proxy=proxy),
            }

        async with httpx.AsyncClient(timeout=30.0, mounts=mounts) as client:
            response = await client.get(api_url)
            response.raise_for_status()

            # è§£æ XML å“åº”
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)

            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„è®ºæ–‡
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entries = root.findall('atom:entry', ns)

            if entries:
                entry = entries[0]
                # è·å– arXiv ID
                arxiv_id_elem = entry.find('atom:id', ns)
                if arxiv_id_elem is not None:
                    arxiv_id_full = arxiv_id_elem.text
                    # æå– ID éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼šhttp://arxiv.org/abs/2410.04618 -> 2410.04618ï¼‰
                    arxiv_id = arxiv_id_full.split('/abs/')[-1]
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"

                    # è·å–æ ‡é¢˜ç¡®è®¤
                    title_elem = entry.find('atom:title', ns)
                    found_title = title_elem.text.strip() if title_elem is not None else "Unknown"

                    elapsed = time.time() - start_time
                    logger.info(
                        "âœ… arXiv æœç´¢æˆåŠŸ",
                        arxiv_id=arxiv_id,
                        found_title=found_title[:100],
                        elapsed_time=f"{elapsed:.2f}s"
                    )

                    return json.dumps({
                        "success": True,
                        "pdf_url": pdf_url,
                        "arxiv_id": arxiv_id,
                        "arxiv_abs_url": f"https://arxiv.org/abs/{arxiv_id}",
                        "found_title": found_title,
                        "message": f"âœ… åœ¨ arXiv æ‰¾åˆ°è®ºæ–‡ï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰\nPDF: {pdf_url}\narXiv ID: {arxiv_id}"
                    }, ensure_ascii=False, indent=2)

        # æœªæ‰¾åˆ°
        elapsed = time.time() - start_time
        logger.warning(
            "âš ï¸ arXiv æœªæ‰¾åˆ°è®ºæ–‡",
            paper_title=paper_title[:100],
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"åœ¨ arXiv æœªæ‰¾åˆ°è®ºæ–‡ã€Š{paper_title}ã€‹ã€‚å¯èƒ½ä¸æ˜¯ arXiv è®ºæ–‡ï¼Œæˆ–æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…ã€‚"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ arXiv æœç´¢å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"arXiv æœç´¢å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def download_pdf_from_url(
    pdf_url: Annotated[str, "PDFæ–‡ä»¶çš„URL"],
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºå‘½åæ–‡ä»¶ï¼‰"] = "paper"
) -> str:
    """
    ä¸‹è½½ PDF å¹¶è¯»å–å…¨éƒ¨å†…å®¹

    å‚æ•°:
        pdf_url: PDF æ–‡ä»¶çš„ URL
        paper_title: è®ºæ–‡æ ‡é¢˜

    è¿”å›:
        åŒ…å« PDF å†…å®¹å’Œå…ƒæ•°æ®çš„ JSON
    """
    global _current_paper
    start_time = time.time()

    try:
        logger.info("ğŸ“¥ å¼€å§‹ä¸‹è½½ PDF", pdf_url=pdf_url[:100], paper_title=paper_title[:50])

        # æ¸…ç†æ–‡ä»¶å
        safe_title = paper_title[:50].replace('/', '_').replace(':', '_').replace('?', '_')
        local_path = PDF_DIR / f"{safe_title}.pdf"

        # ä¸‹è½½ PDF
        proxy = os.getenv('http_proxy')
        mounts = None
        if proxy:
            mounts = {
                "http://": httpx.AsyncHTTPTransport(proxy=proxy),
                "https://": httpx.AsyncHTTPTransport(proxy=proxy),
            }

        async with httpx.AsyncClient(
            timeout=60.0,
            follow_redirects=True,
            mounts=mounts
        ) as client:
            response = await client.get(pdf_url)
            response.raise_for_status()

            with open(local_path, 'wb') as f:
                f.write(response.content)

        # è¯»å– PDF å†…å®¹
        logger.info("ğŸ“– å¼€å§‹è¯»å– PDF å†…å®¹")
        pdf_content, pdf_metadata = _read_pdf_file(str(local_path))

        _current_paper["pdf_path"] = str(local_path)
        _current_paper["pdf_url"] = pdf_url
        _current_paper["pdf_content"] = pdf_content
        _current_paper["pdf_metadata"] = pdf_metadata

        elapsed = time.time() - start_time
        logger.info(
            "âœ… PDF ä¸‹è½½å¹¶è¯»å–æˆåŠŸ",
            file_size=f"{len(response.content) / 1024 / 1024:.2f}MB",
            pages=pdf_metadata.get("pages", 0),
            content_length=len(pdf_content),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "local_path": str(local_path),
            "pdf_url": pdf_url,
            "pdf_content": pdf_content[:5000],  # è¿”å›å‰5000å­—ç¬¦é¢„è§ˆ
            "pdf_metadata": json.dumps(pdf_metadata, ensure_ascii=False),
            "message": f"âœ… PDF ä¸‹è½½å¹¶è¯»å–æˆåŠŸï¼æ–‡ä»¶: {local_path}ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ PDF ä¸‹è½½å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"å¤„ç†å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def read_local_pdf(
    pdf_path: Annotated[str, "PDFæ–‡ä»¶çš„æœ¬åœ°è·¯å¾„"]
) -> str:
    """
    è¯»å–æœ¬åœ° PDF æ–‡ä»¶å†…å®¹

    å‚æ•°:
        pdf_path: PDF æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„

    è¿”å›:
        åŒ…å« PDF å†…å®¹å’Œå…ƒæ•°æ®çš„ JSON
    """
    global _current_paper
    start_time = time.time()

    try:
        logger.info("ğŸ“– å¼€å§‹è¯»å–æœ¬åœ° PDF", pdf_path=pdf_path)

        pdf_content, pdf_metadata = _read_pdf_file(pdf_path)

        _current_paper["pdf_path"] = pdf_path
        _current_paper["pdf_content"] = pdf_content
        _current_paper["pdf_metadata"] = pdf_metadata

        elapsed = time.time() - start_time
        file_size = os.path.getsize(pdf_path) / 1024 / 1024 if os.path.exists(pdf_path) else 0
        logger.info(
            "âœ… æœ¬åœ° PDF è¯»å–æˆåŠŸ",
            pdf_path=pdf_path,
            file_size=f"{file_size:.2f}MB",
            pages=pdf_metadata.get("pages", 0),
            content_length=len(pdf_content),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "pdf_path": pdf_path,
            "pdf_content": pdf_content[:5000],
            "pdf_metadata": json.dumps(pdf_metadata, ensure_ascii=False),
            "message": f"âœ… PDF è¯»å–æˆåŠŸï¼æ–‡ä»¶: {pdf_path}ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ æœ¬åœ° PDF è¯»å–å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"è¯»å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


def _read_pdf_file(pdf_path: str):
    """è¯»å– PDF æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    doc = fitz.open(pdf_path)
    total_pages = doc.page_count

    # æå–å…ƒæ•°æ®
    metadata = doc.metadata
    metadata_dict = {
        "title": metadata.get("title", ""),
        "author": metadata.get("author", ""),
        "subject": metadata.get("subject", ""),
        "keywords": metadata.get("keywords", ""),
        "creator": metadata.get("creator", ""),
        "producer": metadata.get("producer", ""),
        "creationDate": metadata.get("creationDate", ""),
        "modDate": metadata.get("modDate", ""),
        "pages": total_pages,
    }

    # åˆ†æ‰¹è¯»å–å…¨éƒ¨å†…å®¹ï¼ˆæ¯æ¬¡10é¡µï¼‰
    PAGES_PER_BATCH = 10
    all_pdf_text = []
    current_page = 0

    while current_page < total_pages:
        end_page = min(current_page + PAGES_PER_BATCH, total_pages)
        batch_text = ""

        for page_num in range(current_page, end_page):
            page = doc[page_num]
            batch_text += f"\n\n--- Page {page_num + 1} ---\n\n"
            batch_text += page.get_text()

        all_pdf_text.append(batch_text)
        current_page = end_page

    doc.close()

    # åˆå¹¶æ‰€æœ‰å†…å®¹
    full_pdf_content = "".join(all_pdf_text)

    # æˆªæ–­åˆ°åˆç†é•¿åº¦ï¼ˆ~50000å­—ç¬¦ï¼‰
    MAX_CHARS = 50000
    if len(full_pdf_content) > MAX_CHARS:
        pdf_content = full_pdf_content[:MAX_CHARS] + "\n\n[å†…å®¹å·²æˆªæ–­ï¼Œåç»­å†…å®¹ç•¥]"
    else:
        pdf_content = full_pdf_content

    return pdf_content, metadata_dict






@function_tool
async def generate_paper_digest(
    xiaohongshu_content: Annotated[str, "å°çº¢ä¹¦å¸–å­å†…å®¹"] = "",
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜"] = "",
    pdf_content: Annotated[str, "PDFå…¨æ–‡å†…å®¹"] = "",
    authors: Annotated[str, "ä½œè€…åˆ—è¡¨ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²ï¼‰"] = "[]",
    publication_date: Annotated[str, "å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰"] = "",
    venue: Annotated[str, "æœŸåˆŠ/ä¼šè®®åç§°"] = "",
    abstract: Annotated[str, "æ‘˜è¦"] = "",
    affiliations: Annotated[str, "æœºæ„"] = "",
    keywords: Annotated[str, "å…³é”®è¯åˆ—è¡¨ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²ï¼‰"] = "[]",
    project_page: Annotated[str, "é¡¹ç›®ä¸»é¡µ"] = "",
    other_resources: Annotated[str, "å…¶ä»–èµ„æº"] = "",
) -> str:
    """
    ç”Ÿæˆç»“æ„åŒ–è®ºæ–‡æ•´ç†

    å‚æ•°:
        xiaohongshu_content: å°çº¢ä¹¦å¸–å­å†…å®¹
        paper_title: è®ºæ–‡æ ‡é¢˜
        pdf_content: PDF å…¨æ–‡å†…å®¹
        authors: ä½œè€…åˆ—è¡¨
        publication_date: å‘è¡¨æ—¥æœŸ
        venue: æœŸåˆŠ/ä¼šè®®
        abstract: æ‘˜è¦
        affiliations: æœºæ„
        keywords: å…³é”®è¯
        project_page: é¡¹ç›®ä¸»é¡µ
        other_resources: å…¶ä»–èµ„æº

    è¿”å›:
        Markdownæ ¼å¼çš„è®ºæ–‡æ•´ç†
    """
    global _openai_client, _current_paper
    start_time = time.time()

    # è¯»å–æ¨¡æ¿
    template_path = Path(__file__).parent / "digest_template.md"
    with open(template_path, 'r', encoding='utf-8') as f:
        template_content = f.read()

    try:
        logger.info("âœï¸ å¼€å§‹ç”Ÿæˆè®ºæ–‡æ•´ç†ï¼ˆLLM è°ƒç”¨ 2/2ï¼‰", paper_title=paper_title[:100])
        prompt = f"""
ä½ æ˜¯è®ºæ–‡æ•´ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼ŒæŒ‰ç…§æ¨¡æ¿ç”Ÿæˆé«˜è´¨é‡çš„è®ºæ–‡æ•´ç†ã€‚

# è®ºæ–‡åŸºæœ¬ä¿¡æ¯
- **æ ‡é¢˜**: {paper_title}
- **ä½œè€…**: {authors}
- **æœºæ„**: {affiliations}
- **å‘è¡¨æ—¶é—´**: {publication_date}
- **æœŸåˆŠ/ä¼šè®®**: {venue}
- **æ ‡ç­¾**: {keywords}
- **é¡¹ç›®é¡µ**: {project_page if project_page else "[æ— ]"}
- **å…¶ä»–èµ„æº**: {other_resources if other_resources else "[æ— ]"}

# å°çº¢ä¹¦å†…å®¹ï¼ˆå‚è€ƒï¼‰
{xiaohongshu_content}

# è®ºæ–‡æ‘˜è¦
{abstract if abstract else "[æœªæå–åˆ°æ‘˜è¦]"}

# PDF å…¨æ–‡å†…å®¹ï¼ˆé‡ç‚¹å‚è€ƒï¼‰
{pdf_content[:20000] if pdf_content else "[æœªæä¾›PDFå†…å®¹]"}

# æ•´ç†æ¨¡æ¿
{template_content}

# è¦æ±‚ï¼ˆâš ï¸ ä¸¥æ ¼æ‰§è¡Œï¼‰
1. **å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç»“æ„**ï¼ŒåŒ…å«æ‰€æœ‰ç« èŠ‚
2. **ä¼˜å…ˆä½¿ç”¨ PDF å…¨æ–‡å†…å®¹**ï¼Œå…¶æ¬¡å‚è€ƒå°çº¢ä¹¦å†…å®¹
3. **è¯¦ç»†å¡«å……ä»¥ä¸‹ç« èŠ‚**ï¼š
   - æ–‡ç« èƒŒæ™¯ä¸åŸºæœ¬è§‚ç‚¹
   - ç°æœ‰è§£å†³æ–¹æ¡ˆçš„æ€è·¯ä¸é—®é¢˜
   - æœ¬æ–‡æå‡ºçš„æ€æƒ³ä¸æ–¹æ³•
   - æ–¹æ³•å®ç°ç»†èŠ‚
   - æ–¹æ³•æœ‰æ•ˆæ€§è¯æ˜ï¼ˆå®éªŒï¼‰
   - å±€é™æ€§ä¸æœªæ¥æ–¹å‘
4. **å¦‚æœä¿¡æ¯ä¸è¶³**ï¼Œæ˜ç¡®æ ‡æ³¨ "[ä¿¡æ¯ä¸è¶³]"
5. **ä¿æŒå­¦æœ¯æ€§å’Œä¸“ä¸šæ€§**
6. **ä½¿ç”¨ Markdown æ ¼å¼**
7. **åŸºæœ¬ä¿¡æ¯å¿…é¡»å‡†ç¡®å¡«å†™**ï¼ˆåŒ…æ‹¬å®Œæ•´æ—¥æœŸã€æ ‡ç­¾ã€é¡¹ç›®é¡µã€å…¶ä»–èµ„æºï¼‰

è¯·è¾“å‡ºå®Œæ•´çš„è®ºæ–‡æ•´ç†ï¼ˆMarkdownæ ¼å¼ï¼‰ï¼š
"""

        response = await _openai_client.chat.completions.create(
            model="gpt-5-mini",  # ä½¿ç”¨ gpt-5-miniï¼Œå¤§ä¸Šä¸‹æ–‡çª—å£
            messages=[{
                "role": "system",
                "content": "ä½ æ˜¯ä¸“ä¸šçš„è®ºæ–‡æ•´ç†ä¸“å®¶ï¼Œæ“…é•¿ç»“æ„åŒ–æ•´ç†å­¦æœ¯è®ºæ–‡ã€‚ä½ å¿…é¡»è¯¦ç»†ã€å®Œæ•´åœ°å¡«å……è®ºæ–‡æ•´ç†æ¨¡æ¿çš„æ‰€æœ‰ç« èŠ‚ã€‚"
            }, {
                "role": "user",
                "content": prompt
            }],
        )

        digest_content = response.choices[0].message.content

        # ä¿å­˜åˆ°æ–‡ä»¶
        safe_title = paper_title[:50].replace('/', '_').replace(':', '_') if paper_title else "paper"
        output_file = OUTPUT_DIR / f"{safe_title}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(digest_content)

        _current_paper["digest_content"] = digest_content
        _current_paper["digest_file"] = str(output_file)

        elapsed = time.time() - start_time
        logger.info(
            "âœ… è®ºæ–‡æ•´ç†ç”ŸæˆæˆåŠŸ",
            paper_title=paper_title[:100],
            content_length=len(digest_content),
            output_file=str(output_file),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "output_file": str(output_file),
            "digest_content": digest_content,
            "message": f"âœ… è®ºæ–‡æ•´ç†ç”ŸæˆæˆåŠŸï¼æ–‡ä»¶: {output_file}ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ è®ºæ–‡æ•´ç†ç”Ÿæˆå¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


@function_tool
async def save_digest_to_notion(
    paper_title: Annotated[str, "è®ºæ–‡æ ‡é¢˜"],
    digest_content: Annotated[str, "è®ºæ–‡æ•´ç†å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰"],
    source_url: Annotated[str, "æ¥æºURL"] = "",
    pdf_url: Annotated[str, "PDFé“¾æ¥"] = "",
    authors: Annotated[str, "ä½œè€…åˆ—è¡¨ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²æˆ–é€—å·åˆ†éš”ï¼‰"] = "",
    affiliations: Annotated[str, "æœºæ„"] = "",
    publication_date: Annotated[str, "å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰"] = "",
    venue: Annotated[str, "æœŸåˆŠ/ä¼šè®®åç§°"] = "",
    abstract: Annotated[str, "æ‘˜è¦"] = "",
    keywords: Annotated[str, "å…³é”®è¯ï¼ˆJSONæ•°ç»„å­—ç¬¦ä¸²æˆ–é€—å·åˆ†éš”ï¼‰"] = "",
    doi: Annotated[str, "DOI"] = "",
    arxiv_id: Annotated[str, "ArXiv ID"] = "",
    project_page: Annotated[str, "é¡¹ç›®ä¸»é¡µ"] = "",
    other_resources: Annotated[str, "å…¶ä»–èµ„æºï¼ˆä»£ç ä»“åº“ã€æ•°æ®é›†ç­‰ï¼‰"] = "",
) -> str:
    """
    å°†è®ºæ–‡æ•´ç†ä¿å­˜åˆ° Notion

    å‚æ•°:
        paper_title: è®ºæ–‡æ ‡é¢˜
        digest_content: è®ºæ–‡æ•´ç†å†…å®¹
        source_url: æ¥æºé“¾æ¥
        pdf_url: PDFé“¾æ¥
        authors: ä½œè€…
        affiliations: æœºæ„
        publication_date: å‘è¡¨æ—¥æœŸ
        venue: æœŸåˆŠ/ä¼šè®®
        abstract: æ‘˜è¦
        keywords: å…³é”®è¯
        doi: DOI
        arxiv_id: ArXiv ID
        project_page: é¡¹ç›®ä¸»é¡µ
        other_resources: å…¶ä»–èµ„æº

    è¿”å›:
        ä¿å­˜ç»“æœ
    """
    from notion_client import AsyncClient
    start_time = time.time()

    try:
        logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜è®ºæ–‡æ•´ç†åˆ° Notion", paper_title=paper_title[:100])
        client = AsyncClient(auth=os.getenv('NOTION_TOKEN'))

        # æ„å»º properties
        properties = {
            "Name": {
                "title": [{"text": {"content": paper_title[:2000]}}]
            }
        }

        # Authors - å¤„ç† JSON æ•°ç»„æˆ–é€—å·åˆ†éš”å­—ç¬¦ä¸²
        if authors:
            try:
                authors_list = json.loads(authors) if authors.startswith('[') else [a.strip() for a in authors.split(',')]
                authors_str = ", ".join(authors_list)
                properties["Authors"] = {"rich_text": [{"text": {"content": authors_str[:2000]}}]}
            except:
                properties["Authors"] = {"rich_text": [{"text": {"content": authors[:2000]}}]}

        if affiliations:
            properties["Affiliations"] = {"rich_text": [{"text": {"content": affiliations[:2000]}}]}
        if venue:
            properties["Venue"] = {"rich_text": [{"text": {"content": venue[:2000]}}]}

        # Abstract - ä½¿ç”¨ä¸­æ–‡æ‘˜è¦ï¼ˆä» digest_content æå–ï¼‰
        if abstract:
            chinese_abstract = _extract_chinese_abstract(digest_content)
            if chinese_abstract:
                properties["Abstract"] = {"rich_text": [{"text": {"content": chinese_abstract[:2000]}}]}

        # Keywords - multi_select ç±»å‹
        if keywords:
            try:
                keywords_list = json.loads(keywords) if keywords.startswith('[') else [k.strip() for k in keywords.split(',')]
                properties["Keywords"] = {"multi_select": [{"name": kw} for kw in keywords_list[:10]]}  # Notion é™åˆ¶
            except:
                pass

        # DOI
        if doi:
            properties["DOI"] = {"rich_text": [{"text": {"content": doi[:2000]}}]}

        # ArXiv ID
        if arxiv_id:
            properties["ArXiv ID"] = {"rich_text": [{"text": {"content": arxiv_id[:2000]}}]}

        # Publication Date - date ç±»å‹
        if publication_date:
            try:
                # éªŒè¯æ—¥æœŸæ ¼å¼ YYYY-MM-DD
                if len(publication_date) == 10 and publication_date[4] == '-' and publication_date[7] == '-':
                    properties["Publication Date"] = {"date": {"start": publication_date}}
            except:
                pass

        # Other Resources - rich_text ç±»å‹
        if other_resources:
            properties["Other Resources"] = {"rich_text": [{"text": {"content": other_resources[:2000]}}]}

        # è½¬æ¢ Markdown ä¸º Notion blocks
        blocks = _markdown_to_notion_blocks(digest_content)

        response = await client.pages.create(
            parent={"database_id": os.getenv('NOTION_DATABASE_ID')},
            properties=properties,
            children=blocks[:100],
        )

        page_id = response["id"]
        page_url = f"https://notion.so/{page_id.replace('-', '')}"

        await client.aclose()

        elapsed = time.time() - start_time
        logger.info(
            "âœ… è®ºæ–‡æ•´ç†å·²ä¿å­˜åˆ° Notion",
            paper_title=paper_title[:100],
            page_id=page_id,
            page_url=page_url,
            properties_count=len(properties),
            blocks_count=len(blocks[:100]),
            elapsed_time=f"{elapsed:.2f}s"
        )

        return json.dumps({
            "success": True,
            "page_id": page_id,
            "page_url": page_url,
            "message": f"âœ… è®ºæ–‡æ•´ç†å·²ä¿å­˜åˆ° Notionï¼ï¼ˆè€—æ—¶ {elapsed:.2f}sï¼‰\n\næŸ¥çœ‹é“¾æ¥: {page_url}"
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            "âŒ ä¿å­˜åˆ° Notion å¤±è´¥",
            error=str(e),
            elapsed_time=f"{elapsed:.2f}s"
        )
        return json.dumps({
            "success": False,
            "error": f"ä¿å­˜å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)


def _extract_chinese_abstract(digest_content: str) -> str:
    """ä»ç”Ÿæˆçš„ä¸­æ–‡è®ºæ–‡æ•´ç†ä¸­æå–æ‘˜è¦éƒ¨åˆ†"""
    import re

    patterns = [
        r'##\s*ğŸ“\s*æ‘˜è¦\s*\(.*?\)\s*\n+(.*?)(?=\n##|\n---|\Z)',
        r'##\s*æ‘˜è¦\s*\(.*?\)\s*\n+(.*?)(?=\n##|\n---|\Z)',
        r'##\s*æ‘˜è¦\s*\n+(.*?)(?=\n##|\n---|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, digest_content, re.DOTALL)
        if match:
            abstract = match.group(1).strip()
            # Clean markdown formatting
            abstract = re.sub(r'\*\*(.+?)\*\*', r'\1', abstract)
            abstract = re.sub(r'\*(.+?)\*', r'\1', abstract)
            abstract = ' '.join(abstract.split())
            return abstract[:2000]

    # Fallback: ä½¿ç”¨å‰200å­—ç¬¦
    return digest_content[:200].replace('#', '').strip()


def _markdown_to_notion_blocks(markdown_text: str) -> list:
    """
    å°† Markdown è½¬æ¢ä¸º Notion API blocks

    ä½¿ç”¨ mistletoe è§£æ Markdown å¹¶è½¬æ¢ä¸º Notion blocks
    æ”¯æŒï¼šåŠ ç²—ã€æ–œä½“ã€åˆ é™¤çº¿ã€å†…è”ä»£ç ã€åµŒå¥—åˆ—è¡¨ç­‰

    Args:
        markdown_text: Markdown æ–‡æœ¬

    Returns:
        Notion API blocks åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
        from .notion_markdown_converter import markdown_to_notion_blocks

        blocks = markdown_to_notion_blocks(markdown_text)
        return blocks
    except Exception as e:
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨
        import traceback
        print(f"Markdownè½¬æ¢å¤±è´¥: {e}")
        traceback.print_exc()
        return []


# Digest Agent å®šä¹‰
digest_agent = Agent(
    name="digest_agent",
    instructions="""ä½ æ˜¯è®ºæ–‡æ·±åº¦æ•´ç†ä¸“å®¶ï¼ˆDigest Agentï¼‰ã€‚âš¡ ä»…éœ€ 2 æ¬¡ LLM è°ƒç”¨ï¼Œå¤§å¹…åŠ é€Ÿï¼

ä½ çš„èŒè´£æ˜¯æ¥æ”¶è®ºæ–‡ç›¸å…³ä¿¡æ¯ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

## æ‰§è¡Œæµç¨‹ï¼ˆä»…éœ€ 2 æ¬¡ LLM è°ƒç”¨ï¼ï¼‰

**ç¬¬ä¸€é˜¶æ®µï¼šè·å– PDF å¹¶æå–æ‰€æœ‰å…ƒæ•°æ®**
1. **è·å–è®ºæ–‡å†…å®¹**
   - å¦‚æœæä¾›äº†å°çº¢ä¹¦ URLï¼Œä½¿ç”¨ fetch_xiaohongshu_post è·å–å†…å®¹
   - å¦‚æœæä¾›äº† PDF URLï¼Œä½¿ç”¨ download_pdf_from_url ä¸‹è½½
   - å¦‚æœæä¾›äº†æœ¬åœ° PDF è·¯å¾„ï¼Œä½¿ç”¨ read_local_pdf è¯»å–

2. **æœç´¢è®ºæ–‡ PDF**ï¼ˆå¦‚æœæ²¡æœ‰æä¾› PDF URLï¼‰
   - ä¼˜å…ˆä½¿ç”¨ search_arxiv_pdf åœ¨ arXiv æœç´¢è®ºæ–‡
   - ä»æœç´¢ç»“æœä¸­è·å– PDF URL å’Œ arXiv ID

3. **âš¡ ä¸€æ¬¡ LLM è°ƒç”¨æå–æ‰€æœ‰å…ƒæ•°æ®**ï¼ˆæ›¿ä»£æ—§çš„ä¸¤ä¸ªå•ç‹¬è°ƒç”¨ï¼‰
   - ä½¿ç”¨ extract_paper_metadata **ä¸€æ¬¡è°ƒç”¨åŒæ—¶æå–**ï¼š
     * è®ºæ–‡æ ‡é¢˜ï¼ˆtitleï¼‰
     * ä½œè€…ï¼ˆauthorsï¼‰
     * å‘è¡¨æ—¥æœŸï¼ˆpublication_dateï¼ŒYYYY-MM-DDï¼‰
     * æœŸåˆŠ/ä¼šè®®ï¼ˆvenueï¼‰
     * æ‘˜è¦ï¼ˆabstractï¼‰
     * æœºæ„ï¼ˆaffiliationsï¼‰
     * å…³é”®è¯ï¼ˆkeywordsï¼‰
     * DOI
     * ArXiv ID
     * é¡¹ç›®é¡µï¼ˆproject_pageï¼‰
     * å…¶ä»–èµ„æºï¼ˆother_resourcesï¼‰
   - âš ï¸ å¿…é¡»ä¼ å…¥ xiaohongshu_contentã€pdf_contentã€pdf_metadata

**ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆè®ºæ–‡æ•´ç†å¹¶ä¿å­˜**
4. **âš¡ ä¸€æ¬¡ LLM è°ƒç”¨ç”Ÿæˆå®Œæ•´è®ºæ–‡æ•´ç†**
   - ä½¿ç”¨ generate_paper_digest ç”Ÿæˆç»“æ„åŒ–çš„ä¸­æ–‡è®ºæ–‡æ•´ç†
   - ä¼ å…¥æ‰€æœ‰æå–çš„å…ƒæ•°æ®
   - å¿…é¡»åŒ…å«æ‰€æœ‰æ¨¡æ¿ç« èŠ‚ï¼Œå†…å®¹è¯¦ç»†

5. **ä¿å­˜åˆ° Notion**
   - ä½¿ç”¨ save_digest_to_notion ä¿å­˜æ•´ç†å†…å®¹å’Œæ‰€æœ‰å…ƒæ•°æ®
   - âš ï¸ **å¿…é¡»ä¼ é€’ extract_paper_metadata è¿”å›çš„æ‰€æœ‰å­—æ®µ**ï¼š
     * paper_title - è®ºæ–‡æ ‡é¢˜
     * digest_content - ç”Ÿæˆçš„è®ºæ–‡æ•´ç†å†…å®¹
     * source_url - å°çº¢ä¹¦ URLï¼ˆå¦‚æœæœ‰ï¼‰
     * pdf_url - PDF é“¾æ¥
     * authors - ä½œè€…åˆ—è¡¨ï¼ˆJSON æ•°ç»„å­—ç¬¦ä¸²ï¼‰
     * affiliations - æœºæ„
     * publication_date - å®Œæ•´å‘è¡¨æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
     * venue - æœŸåˆŠ/ä¼šè®®åç§°
     * abstract - è‹±æ–‡æ‘˜è¦
     * keywords - å…³é”®è¯åˆ—è¡¨ï¼ˆJSON æ•°ç»„å­—ç¬¦ä¸²ï¼‰
     * doi - DOIï¼ˆå¦‚æœæœ‰ï¼‰
     * arxiv_id - ArXiv IDï¼ˆå¦‚æœæœ‰ï¼‰
     * project_page - é¡¹ç›®ä¸»é¡µï¼ˆå¦‚æœæœ‰ï¼‰
     * other_resources - å…¶ä»–èµ„æºï¼ˆå¦‚æœæœ‰ï¼‰

âš ï¸ å…³é”®è¦æ±‚ï¼š
- âœ… **åªè¿›è¡Œ 2 æ¬¡ LLM è°ƒç”¨**ï¼ˆextract_paper_metadata ä¸€æ¬¡ï¼Œgenerate_paper_digest ä¸€æ¬¡ï¼‰
- âœ… ä¸å†ä½¿ç”¨å·²åˆ é™¤çš„å‡½æ•°
- âœ… å¿…é¡»ä¸¥æ ¼æŒ‰ç…§é¡ºåºæ‰§è¡Œ
- âœ… æ¯ä¸ªæ­¥éª¤éƒ½è¦æ£€æŸ¥ç»“æœæ˜¯å¦æˆåŠŸ
- âœ… å…ƒä¿¡æ¯å¿…é¡»å‡†ç¡®ä¸”å®Œæ•´
- âœ… è°ƒç”¨ save_digest_to_notion æ—¶ä¼ é€’æ‰€æœ‰å­—æ®µï¼ˆç‰¹åˆ«æ˜¯ authors å’Œ keywords è¦è½¬ä¸º JSON æ•°ç»„å­—ç¬¦ä¸²ï¼‰
- âŒ å¦‚æœæŸä¸ªæ­¥éª¤å¤±è´¥ï¼ŒæŠ¥å‘Šé”™è¯¯å¹¶åœæ­¢

ä½ ä¸“æ³¨äºè®ºæ–‡æ•´ç†å·¥ä½œï¼Œä¸å¤„ç†å®šæ—¶ä»»åŠ¡ç›¸å…³çš„äº‹æƒ…ã€‚
""",
    model="gpt-5",
    tools=[
        fetch_xiaohongshu_post,
        search_arxiv_pdf,
        download_pdf_from_url,
        read_local_pdf,
        extract_paper_metadata,  # âœ¨ æ–°çš„åˆå¹¶å‡½æ•°ï¼ˆæ›¿ä»£æ—§çš„ä¸¤ä¸ªï¼‰
        generate_paper_digest,
        save_digest_to_notion,
    ]
)
